{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMmju/+Dts4LVloQYB8uCZg","gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Unsloth Problem 2 - Make `QLoRA` work with `FSDP2` \n\nThis is the **Kaggle 2xT4** notebook, run for 60 steps.","metadata":{}},{"cell_type":"markdown","source":"---\n---\n---\n<a name=\"FSDP2\"></a>\n## B) Make `QLoRA` work with `FSDP2` [Difficulty: Medium to Hard] [Max points: 10]\n\n1. Goal: Write a single Python script to finetune Llama 3.1 8B on 2x or more GPUs with FSDP2.\n\n2. You must showcase this working in a free **Kaggle notebook with 2 x Tesla T4 GPUs**.\n\n3. Pipeline parallelism is also fine, but must utilize [`zero bubble scheduling`](https://pytorch.org/docs/stable/distributed.pipelining.html#torch.distributed.pipelining.schedules.ScheduleInterleavedZeroBubble) somehow.\n\n4. Can use a pre-quantized 4bit BnB safetensor file from [Unsloth's HF page](https://huggingface.co/unsloth) or a full 16bit one, but must do QLoRA.\n\n5. Can use `accelerate` but must be FSDP2 or related - you can investigate https://github.com/huggingface/accelerate/pull/3394, Torch Titan, other repos etc.\n\n6. Must be fully `transformers` compatible - so we must use `TrainingArguments` and `Trainer`, or `TRL` related classes.\n\n7. The loss must be equivalent to single GPU training.\n\n8. You must enable all features in FSDP2 - ie showcase offloading, checkpointing, mixed precision training etc.\n\n9. You can use `nf4` from `torch AO`, but best from `bitsandbytes`.\n\n10. Finally showcase everything working in a free Kaggle 2x Tesla T4 notebook.","metadata":{"id":"uXshnajO44Kb"}},{"cell_type":"code","source":"# Code to install Unsloth, Triton, Torch etc\n\n# For FSDP2 we need torch >= 2.6.0 (Many version incompatabilities, but we can hack something together)\n# NOTE: Many sources suggest FSDP2 is supported as of torch>=2.5.1, but it may have been experimental then?\n!pip install --no-cache-dir --force-reinstall datasets huggingface_hub hf_transfer\n!pip install --no-cache-dir --force-reinstall bitsandbytes peft trl matplotlib\n\n# Need some changes to `accelerate` to support FSDP2 in accelerate\n!pip install --no-cache-dir git+https://github.com/byi8220/accelerate.git@experimental/qlora-fsdp2\n\n# Torch 2.6.0 for FSDP2\n!pip install --no-cache-dir --force-reinstall torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\n\n# Upgrading torch leads to version breakages relating to numpy, transformers, scipy\n# https://github.com/scipy/scipy/issues/21014\n!pip install --no-cache-dir --force-reinstall numpy==1.26.1\n\n# Install from head to get access to https://github.com/huggingface/transformers/pull/37147\n!pip install --no-cache-dir git+https://github.com/huggingface/transformers.git \n!pip install --no-cache-dir --force-reinstall scipy==1.11.2\n\n!pip install ipywidgets # Needed to export to github","metadata":{"executionInfo":{"elapsed":30396,"status":"ok","timestamp":1742414114585,"user":{"displayName":"Brian Yi","userId":"16364797669375947161"},"user_tz":240},"id":"F_rx9FYMOc2T"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# We must set env vars before importing torch\nimport os\nos.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,\"\\\n    \"roundup_power2_divisions:[32:256,64:128,256:64,>:32]\"\nos.environ[\"NCCL_P2P_DISABLE\"] = \"1\"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## FSDP2 Enabled Model\nWe are launching the script in https://www.kaggle.com/code/byi8220/fsdp2-qlora-py/notebook via `accelerate`.\n\nThe choice to use `accelerate` stems from the requirement that the solution \"Must be fully `transformers` compatible - so we must use `TrainingArguments` and `Trainer`, or `TRL` related classes.\"\n\nSince `TRL` uses an `accelerator` under the hood (see [TRL docs](https://github.com/huggingface/trl/blob/main/docs/source/distributing_training.md)), the least invasive option appears to be to interop with `accelerate`.","metadata":{}},{"cell_type":"code","source":"%%writefile fsdp_config.yaml\n# Since we are doing FSDP (even though it's multi-GPU), we need to specify the distributed type as FSDP\ndistributed_type: FSDP\n# Can be one of \"no\", \"fp16\", or \"bf16\" (see `transformer_engine.yaml` for `fp8`, but it works for FSDP as well)\nmixed_precision: 'fp16'\n# Specify the number of GPUs to use\nnum_processes: 2\n# Then we can specify the FSDP config\nfsdp_config:\n  fsdp_activation_checkpointing: true\n  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP\n  fsdp_backward_prefetch: BACKWARD_PRE\n  fsdp_cpu_ram_efficient_loading: true # Required for Qlora https://github.com/huggingface/accelerate/issues/1620\n  fsdp_forward_prefetch: false\n  fsdp_offload_params: false\n  fsdp_sharding_strategy: FULL_SHARD\n  fsdp_state_dict_type: SHARDED_STATE_DICT\n  fsdp_sync_module_states: true\n  fsdp_use_orig_params: true\n  # fsdp_version: 2 # We are converting from fsdp1->fsdp2","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Porting Params4Bit tensors into a DTensor\n\nFSDP2 handles parameter sharding by converting ordinary tensors into `DTensor` objects. DTensors are a *Tensor Parallel* abstraction, where an individual tensor\nis partitioned into shards.\n\nFrom the docstring for `fully_shard`:\n```\n    At initialization, FSDP shards the module's parameters across the data\n    parallel workers given by ``mesh``. Before forward, FSDP all-gathers the\n    sharded parameters across the data-parallel workers to get the unsharded\n    parameters for forward computation. If ``reshard_after_forward`` is\n    ``True``, then FSDP frees the unsharded parameters after forward and\n    re-all-gathers them in backward before gradient computation.\n```\nIn other words, for any worker, its parameters live in distributed space, however is fully colocated for any computation. This means that in theory, we can\ndequantize and then matmul using a quantized parameter by letting FSDP2 be responsible for materializing the full tensor beforehand.","metadata":{}},{"cell_type":"code","source":"%%writefile train.py\n# Get FSDP2 working\n# FSDP2 interface (https://pytorch.org/docs/stable/distributed.fsdp.fully_shard.html)\n\n# Patching huggingface repos is a mess. I needed to hack a patch into `Trainer._inner_training_loop`, which is not clean\nimport os \nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.distributed.fsdp import fully_shard, CPUOffloadPolicy, MixedPrecisionPolicy\nfrom torch.distributed.device_mesh import init_device_mesh\nfrom accelerate import notebook_launcher, FullyShardedDataParallelPlugin\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import get_peft_model, LoraConfig, TaskType, PeftModel\nimport torch\nimport time\nimport transformers.trainer\nfrom bitsandbytes.nn.modules import Linear4bit as bnb_Linear4bit\nfrom peft.tuners.lora.bnb import Linear4bit as peft_Linear4bit\n\n# FSDP2 shards parameters across devices, which interferes with quantized parameters.\n# The naive solution to this problem is to not shard quantized parameters across multiple GPUs\ndef patch_fsdp():\n    from torch.distributed.fsdp._fully_shard._fsdp_param import FSDPParam\n    _init_dtype_attrs = FSDPParam.init_dtype_attrs\n\n    # Patch init_dtype_attrs to not cast quantized types to float\n    def _patched_init_dtype_attrs(self, mp_policy):\n        param_dtype, reduce_dtype = (mp_policy.param_dtype, mp_policy.reduce_dtype)\n        self.orig_dtype = self.sharded_param.dtype\n        # Clamp `param_dtype` to `None` if no casting is required\n        if param_dtype == self.orig_dtype:\n            param_dtype = None\n\n        # Quantized int types shouldn't be cast to float \n        int_types = [torch.uint8, torch.uint16, torch.uint32, torch.uint64]\n        if self.orig_dtype in int_types:\n            param_dtype = None\n            quant_dtype = self.orig_dtype\n\n        self.param_dtype = param_dtype\n        self.reduce_dtype = reduce_dtype\n        # None indicates that the mixed precision is not enabled\n\n\n    # Patch `init_sharded_param` to initialize sharded params without gradient.\n    # Tbh there is only a single line of diff between this and the baseline init_sharded_param. \n    # I raised a question in pytorch forums about this as well since it bothered me enough: https://discuss.pytorch.org/t/very-small-stupid-question-about-fsdpparam-init-sharded-param/218875\n    FSDPParam.init_dtype_attrs = _patched_init_dtype_attrs\n\n    # Include imports for patching.\n    import inspect\n    import itertools\n    from dataclasses import dataclass, field\n    from enum import auto, Enum\n    from typing import Any, Callable, cast, List, Optional, Sequence, Tuple\n    \n    import torch\n    import torch.nn as nn\n    from torch._prims_common import make_contiguous_strides_for\n    from torch.distributed._functional_collectives import AsyncCollectiveTensor\n    from torch.distributed.tensor import DTensor, Replicate, Shard\n    from torch.distributed.tensor._dtensor_spec import DTensorSpec, TensorMeta\n    from torch.distributed.tensor.device_mesh import _mesh_resources\n    from torch.distributed.tensor.placement_types import _StridedShard, Placement\n    from torch.distributed.fsdp._fully_shard._fsdp_api import CPUOffloadPolicy, MixedPrecisionPolicy, OffloadPolicy\n    from torch.distributed.fsdp._fully_shard._fsdp_common import (\n        _chunk_with_empty,\n        _from_local_no_grad,\n        _get_dim_chunked_size,\n        _raise_assert_with_print,\n        _to_dtype_if_needed,\n        compiled_autograd_enabled,\n        FSDPMeshInfo,\n        HSDPMeshInfo,\n    )\n    from torch.distributed.fsdp._fully_shard._fsdp_param import (\n        ShardedState,\n        ParamModuleInfo,\n        ExtensionsData,\n    )\n    @torch.no_grad()\n    def _patched_init_sharded_param(\n        self,\n        param: nn.Parameter,\n        device: torch.device,\n        shard_placement_fn: Optional[Callable],\n    ):\n        if param.device != device and param.device.type != \"meta\":\n            raise AssertionError(\n                f\"Expects the parameter to already be moved to device {device} but got {param.device}\"\n            )\n        if not param.is_contiguous():\n            raise NotImplementedError(\n                f\"FSDP does not support non-contiguous parameters yet: {param.shape=} {param.stride()=}\"\n            )\n        fsdp_placement = shard_placement_fn(param) if shard_placement_fn else None\n        if fsdp_placement is None:\n            fsdp_placement = Shard(0)\n        elif fsdp_placement.dim < 0:\n            fsdp_placement = Shard(fsdp_placement.dim + param.ndim)\n        assert isinstance(fsdp_placement, Shard), f\"{fsdp_placement}\"\n        self.fsdp_placement = fsdp_placement\n        shard_dim = fsdp_placement.dim\n        # TODO: Replace the sharded DTensor parameter construction logic with\n        # `distribute_tensor` after https://github.com/pytorch/pytorch/issues/116101\n        # TODO: Simplify the following sharded parameter padding logic after\n        # https://github.com/pytorch/pytorch/issues/113045\n        self.is_dtensor = isinstance(param, DTensor)\n        if self.is_dtensor:\n            self._tp_spec = cast(DTensor, param)._spec\n            dp_mesh, tp_mesh = (self.mesh_info.mesh, self._tp_spec.mesh)\n            dp_global_mesh = _mesh_resources.get_root_mesh(dp_mesh)\n            tp_global_mesh = _mesh_resources.get_root_mesh(tp_mesh)\n            if dp_global_mesh != tp_global_mesh or (\n                dp_global_mesh is None or tp_global_mesh is None\n            ):\n                raise AssertionError(\n                    \"FSDP requires the DP and TP mesh to have the same parent mesh but got: \\n\"\n                    f\"DP's global mesh: {dp_global_mesh}\\nTP's global mesh: {tp_global_mesh}\"\n                )\n            name_dims_error = \"FSDP requires named DeviceMesh dims for ND parallelism\"\n            assert dp_mesh.mesh_dim_names is not None, name_dims_error\n            assert tp_mesh.mesh_dim_names is not None, name_dims_error\n            submesh_names = dp_mesh.mesh_dim_names + tp_mesh.mesh_dim_names\n            self._spmd_mesh = dp_global_mesh[submesh_names]\n            if len(self._tp_spec.placements) != 1:\n                raise NotImplementedError(\n                    f\"FSDP only supports 1D TP, not {self._tp_spec.placements}\"\n                )\n            split_factor = self._tp_spec.num_shards_map[shard_dim]\n            assert (\n                2 <= self._spmd_mesh.ndim <= 3\n            ), f\"_spmd_mesh.ndim can only be 2 or 3 but got {self._spmd_mesh.ndim}.\"\n            self._spmd_placements: Tuple[Placement, ...]\n            dp_shard_tp_placement = (\n                (\n                    _StridedShard(shard_dim, split_factor=split_factor)\n                    if split_factor > 1\n                    else fsdp_placement\n                ),\n                self._tp_spec.placements[0],\n            )\n            if self._spmd_mesh.ndim == 2:\n                self._spmd_placements = dp_shard_tp_placement\n            else:\n                assert self.mesh_info.replicate_mesh_dim == 0\n                self._spmd_placements = (Replicate(),) + dp_shard_tp_placement\n            self._sharding_spec = DTensorSpec(\n                self._spmd_mesh,\n                self._spmd_placements,\n                tensor_meta=self._tp_spec.tensor_meta,\n            )\n            # TODO: Enable uneven sharding for FSDP+TP.\n            if split_factor > 1:  # FSDP has strided sharding on tensor dim 0\n                num_shards = self._sharding_spec.num_shards_map[0]\n                tensor_size_dim_0 = self._sharding_spec.shape[0]\n                if tensor_size_dim_0 % num_shards != 0:\n                    raise NotImplementedError(\n                        \"FSDP+TP sharding does not support uneven sharding for now: \"\n                        f\"tensor dim 0 has size {tensor_size_dim_0} which cannot be \"\n                        f\"evenly sharded into {num_shards} shards.\"\n                    )\n            param_data = cast(DTensor, param)._local_tensor\n        else:\n            self._spmd_mesh = self.mesh_info.mesh\n            if isinstance(self.mesh_info, HSDPMeshInfo):\n                self._spmd_placements = (Replicate(), fsdp_placement)\n            else:\n                self._spmd_placements = (fsdp_placement,)\n            self._sharding_spec = DTensorSpec(\n                self._spmd_mesh,\n                self._spmd_placements,\n                tensor_meta=TensorMeta(param.size(), param.stride(), param.dtype),\n            )\n            param_data = param\n        assert param_data.is_contiguous(), f\"{param_data.shape=} {param_data.stride()=}\"\n        shard_dim = fsdp_placement.dim\n        if shard_dim >= param_data.ndim:\n            raise AssertionError(\n                f\"Shard dim {shard_dim} is invalid for {param_data.ndim}D tensor: {param.shape}\"\n            )\n        self._orig_size = param_data.size()\n        self._contiguous_orig_stride = make_contiguous_strides_for(self._orig_size)\n        shard_rank = self.mesh_info.shard_mesh_rank\n        shard_world_size = self.mesh_info.shard_mesh_size\n        if shard_dim > 0 and param_data.size(shard_dim) % shard_world_size != 0:\n            # If sharding on nonzero dim, require even sharding for now because\n            # the uneven sharding (1) requires extra copies before/after FSDP\n            # collectives and (2) introduces extra complexity to handle padding\n            # and unpadding\n            raise NotImplementedError(\n                f\"FSDP does not support uneven sharding on dim {shard_dim}: \"\n                f\"{param_data.size()} (world size: {shard_world_size})\"\n            )\n        chunks = _chunk_with_empty(param_data, shard_world_size, dim=shard_dim)\n        sharded_param = chunks[shard_rank]\n        self.sharded_size = _get_dim_chunked_size(\n            sharded_param, param_data.size(), dim=shard_dim\n        )\n        self.contiguous_sharded_stride = make_contiguous_strides_for(self.sharded_size)\n        padded_sharded_size = chunks[0].size()  # 0th always padded\n        self.padded_sharded_param_size = padded_sharded_size\n        # Pre-pad the sharded parameter to avoid padding before all-gather\n        padded_sharded_param = param_data.new_zeros(padded_sharded_size)\n        if sharded_param.numel() > 0:\n            padded_sharded_param.narrow(\n                dim=shard_dim, start=0, length=sharded_param.size(shard_dim)\n            ).copy_(sharded_param)\n        if self.offload_to_cpu and not padded_sharded_param.is_meta:\n            padded_sharded_param = padded_sharded_param.cpu()\n            if self.pin_memory:\n                padded_sharded_param = padded_sharded_param.pin_memory(\n                    device=self.device\n                )\n        self._sharded_param_data = padded_sharded_param.view(-1)\n        length = sharded_param.size(shard_dim) if sharded_param.numel() > 0 else 0\n        sharded_param = padded_sharded_param.narrow(\n            dim=shard_dim, start=0, length=length\n        )\n        assert sharded_param.is_contiguous(), f\"{self.fsdp_placement=}\"\n        self.sharded_param = nn.Parameter(self.to_sharded_dtensor(sharded_param), requires_grad=param.requires_grad) # Literally a ONE line diff!\n        self.sharded_param.requires_grad_(param.requires_grad)\n        # Let `param_data` be freed normally when its ref count reaches 0 when\n        # the `fully_shard` call returns to allow provided parameters to alias\n        self._setattr_on_modules(self.sharded_param)\n        self.sharded_state = ShardedState.SHARDED\n\n    _init_sharded_param = FSDPParam._init_sharded_param\n    FSDPParam._init_sharded_param = _patched_init_sharded_param\n    \nimport os\ndef fsdp_main(world_size):\n    # For reproducability between training runs\n    import random\n    import numpy as np\n    os.environ[\"PYTHONHASHSEED\"] = \"3407\"\n    random.seed(3407)\n    np.random.seed(3407)\n    torch.manual_seed(3407)\n    torch.cuda.manual_seed(3407)\n    torch.cuda.manual_seed_all(3407)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    patch_fsdp()\n    mp_policy = os.environ.get(\"ACCELERATE_MIXED_PRECISION\", \"fp16\")\n    print(\"mp_policy:\", mp_policy)\n    rank = os.environ.get(\"LOCAL_RANK\", \"0\")\n    torch.cuda.set_device(int(rank))\n    max_seq_length = 2048\n    model_name = \"unsloth/meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n    param_type = torch.float32 # Everything in accelerate fsdp2 is upcast to float32\n    # if mp_policy == \"fp16\": param_type = torch.float16\n    if mp_policy == \"bf16\": param_type = torch.bfloat16\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit              = True,\n        bnb_4bit_use_double_quant = True,\n        bnb_4bit_quant_type       = \"nf4\",\n        bnb_4bit_compute_dtype    = param_type,\n        llm_int8_skip_modules = [\"lm_head\", \"multi_modal_projector\", \"merger\", \"modality_projection\"],\n    )\n    print(\"Visible devices\", os.environ.get(\"CUDA_VISIBLE_DEVICES\"))\n    device_map = {\"\": torch.cuda.current_device()}\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        device_map=device_map,\n        attn_implementation = \"sdpa\",\n        quantization_config = bnb_config,\n    )\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    tokenizer.padding_side = \"right\"\n    \n    lora_config = LoraConfig(\n        r = 64,\n        lora_alpha = 128,\n        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                          \"gate_proj\", \"up_proj\", \"down_proj\"],\n        lora_dropout = 0,\n        bias = \"none\",\n        task_type = TaskType.CAUSAL_LM,\n    )\n\n    # Get LoRA and setup model\n    model = get_peft_model(model, lora_config)\n\n    int_count = 0\n    nonint_count = 0\n    int_bytes = 0\n    nonint_bytes = 0\n    for n, p in model.named_parameters():\n        if p.dtype == torch.uint8:\n            int_bytes += p.numel() * p.element_size()\n            int_count += 1\n        else:\n            nonint_bytes += p.numel() * p.element_size()\n            nonint_count += 1\n    print(int_count, nonint_count)\n    print(int_bytes / (1024 ** 3), nonint_bytes / (1024 ** 3))\n    with torch.no_grad():\n        for name, param in model.named_parameters():\n            if \".lora_A.\" in name or \".lora_B.\" in name: param.requires_grad_(True)\n            else: param.requires_grad_(False)\n\n    model.gradient_checkpointing_enable()\n    model.enable_input_require_grads()\n    # Get dataset\n    from datasets import load_dataset\n    from trl import SFTTrainer, SFTConfig\n    url = \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"\n    dataset = load_dataset(\"json\", data_files = {\"train\" : url}, split = \"train[:10%]\")\n    \n    assert torch.cuda.device_count() == 2\n    torch.cuda.empty_cache()\n    torch.cuda.reset_peak_memory_stats()\n    trainer = SFTTrainer(\n        model = model,\n        train_dataset = dataset,\n        processing_class = tokenizer,\n        args = SFTConfig(\n            per_device_train_batch_size = 2,\n            gradient_accumulation_steps = 4 // world_size, # 2 GPU x 2 steps = 1 GPU x 4 steps\n            warmup_steps = 1,\n            max_steps = 60,\n            logging_steps = 1,\n            output_dir = f\"outputs-{world_size}xGPU\",\n            overwrite_output_dir = True, \n            seed = 3407,\n            max_seq_length = max_seq_length,\n            fp16 = mp_policy == \"fp16\",  # These seem to override the `accelerate` config's MP policy.\n            bf16 = mp_policy == \"bf16\",  # Maybe worth looking into/raising an issue with the TRL/transformers team.\n            report_to = \"none\", # For W&B\n            dataset_num_proc = 4,\n            average_tokens_across_devices = world_size > 1,\n            gradient_checkpointing_kwargs={'use_reentrant':False},\n            ddp_find_unused_parameters = False,\n        ),\n    )\n    trainer.train()\n    print(\"Memory summary after\")\n    print(torch.cuda.memory_summary())\n    # Because colab, kaggle, and github notebook implementations are not uniform...\n    from ipywidgets import Widget\n    Widget.close_all()\n\nimport os\nworld_size = int(os.environ.get(\"WORLD_SIZE\", 1))\nfsdp_main(world_size)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Launch external script through accelerate with fsdp2 support passed via CLI flag.\n!accelerate to-fsdp2 --config_file /kaggle/working/fsdp_config.yaml --output_file /kaggle/working/fsdp2_config.yaml --overwrite\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!accelerate launch --config_file /kaggle/working/fsdp2_config.yaml /kaggle/working/train.py\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Note: Peak memory usage is rather high due to a known bug with weight loading in `accelerate`, https://github.com/huggingface/accelerate/pull/3482\n\nWe can see from the stats below that the actual distributed training saves memory during training. At the moment, these savings are sadly ineffective due to our high peak. I have some working ideas on how to fix this, however at the time of writing this it seems it may not be so easy, as it would require moving the quantized params to and from a meta tensor, which would involve messing around with `Params4bit.__torch_dispatch__` in ways I am not very familiar with.\n\nThis notebook was meant to be an initial first look at this problem, so I am presenting an MVP without a fix for the above issue. Unless I'm missing something obvious, getting this feature productionized to spec (i.e. must use bnb qlora, fsdp2 with all features, hf trainer) requires sending out multiple patches to every single one of these repos. In addition, there are probably many aspects I've missed, and the performance of this toy prototype leaves much room for improvement (My use of fully_shard is nowhere near optimal, for example).\n\nIn my opinion this feature is actually pretty large and has a lot of small subgoals that need to all be met. Maybe next month if I have time and their support I could work with them on getting a production grade version of this feature out :P","metadata":{}},{"cell_type":"markdown","source":"Now let's compare with a single threaded training run","metadata":{}},{"cell_type":"code","source":"!LOCAL_RANK=0 RANK=0 WORLD_SIZE=1 MASTER_ADDR=0 MASTER_PORT=0 python /kaggle/working/train.py","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#@title Show and export loss curve for 2x GPU\nimport matplotlib.pyplot as plt\nimport json\ntrain_loss = []\nwith open(\"/kaggle/working/outputs-2xGPU/checkpoint-60/trainer_state.json\", \"r\") as f:\n    trainer_state = json.load(f)\nfor elem in trainer_state[\"log_history\"]:\n    if 'loss' in elem.keys():\n        train_loss.append(elem['loss'])\nplt.plot(train_loss)\nplt.xlabel(\"Step\")\nplt.ylabel(\"Training Loss\")\nplt.title(f\"Loss curve when using 2 GPUs\")\nplt.savefig(f\"llama_8b_2x_GPU_loss_curve.png\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#@title Show and export loss curve for 1x GPU\nimport matplotlib.pyplot as plt\nimport json\ntrain_loss = []\nwith open(\"/kaggle/working/outputs-1xGPU/checkpoint-60/trainer_state.json\", \"r\") as f:\n    trainer_state = json.load(f)\nfor elem in trainer_state[\"log_history\"]:\n    if 'loss' in elem.keys():\n        train_loss.append(elem['loss'])\nplt.plot(train_loss)\nplt.xlabel(\"Step\")\nplt.ylabel(\"Training Loss\")\nplt.title(f\"Loss curve when using 1 GPU\")\nplt.savefig(f\"llama_8b_1x_GPU_loss_curve.png\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Because colab, kaggle, and github notebook implementations are not uniform...\nfrom ipywidgets import Widget\nWidget.close_all()","metadata":{},"outputs":[],"execution_count":null}]}