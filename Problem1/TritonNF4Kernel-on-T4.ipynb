{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/byi8220/unsloth-puzzles/blob/main/Problem1/TritonNF4Kernel-on-T4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrBg_fkWSKoe"
      },
      "source": [
        "# Unsloth Problem 1 - Convert nf4 to Triton\n",
        "\n",
        "Run on a Tesla T4 colab instance\n",
        "\n",
        "(Note: Tesla T4 does not support `bfloat16`. Since we must use a T4, we can only do regular `float16`.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoE2DGRZG2Ng"
      },
      "source": [
        "## Problem Statement\n",
        "---\n",
        "---\n",
        "---\n",
        "<a name=\"NF4\"></a>\n",
        "## A) Convert `nf4` to Triton. [Difficulty: Hard] [Max points: 14]\n",
        "\n",
        "1. Goal: Convert a `nf4` quantized tensor into `fp16` or `bf16` into a *single* Triton kernel The double dequant of the `absmax` and weight forming must be done in 1 Triton kernel. Must work on Tesla T4.\n",
        "2. Must be faster than Unsloth's `fast_dequantize` by 1.15x or more, and not use large intermediate memory buffers.\n",
        "3. Must not use `torch.compile`, but can use `trace.enabled` to help on writing Triton kernels.\n",
        "4. Good material: [Unsloth `fast_dequantize` function](https://github.com/unslothai/unsloth/blob/main/unsloth/kernels/utils.py#L128), also [bitsandbytes `dequantize_blockwise`](https://github.com/bitsandbytes-foundation/bitsandbytes/blob/86b6c37a8ad448230cedb60753f63150b603a112/bitsandbytes/functional.py#L958)\n",
        "5. Use `test_dequantize_function` to test your implementation.\n",
        "6. No CUDA allowed. Custom CUDA inside of the Triton is allowed.\n",
        "7. Watch Tim's videos on Youtube: [8-bit Optimizers](https://www.youtube.com/watch?v=2ETNONas068)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_rx9FYMOc2T"
      },
      "outputs": [],
      "source": [
        "# Code to install Unsloth, Triton, Torch etc\n",
        "%%capture\n",
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl\n",
        "!pip install triton==3.1.0 # (https://github.com/unslothai/unsloth/issues/1604)\n",
        "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "\n",
        "!pip install --no-deps unsloth==2025.3.4 # Stick to stable version\n",
        "!pip install --no-deps unsloth_zoo==2025.3.4 # Stick to stable version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EI_d4FLkR51i",
        "outputId": "ffa4e208-999d-4851-d216-1a436190c15d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:xformers:WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
            "    PyTorch 2.5.1+cu121 with CUDA 1201 (you have 2.6.0+cu124)\n",
            "    Python  3.11.11 (you have 3.11.11)\n",
            "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
            "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
            "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ],
      "source": [
        "# Unsloth yells at me to import it before transformers.\n",
        "import unsloth\n",
        "\n",
        "# Helpful functions used through the entire notebook\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import set_seed\n",
        "import time\n",
        "import inspect\n",
        "import os\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "HAS_BFLOAT16 = (major_version >= 8)\n",
        "from inspect import currentframe as _C, getframeinfo\n",
        "_F = lambda c: getframeinfo(c).lineno # Gets line number\n",
        "WARN = lambda x: print(f\"\\033[31m{x}\\033[0m\") # Red colored warnings\n",
        "\n",
        "# https://stackoverflow.com/questions/18425225/getting-the-name-of-a-variable-as-a-string\n",
        "def NAME(var):\n",
        "    callers_local_vars = inspect.currentframe().f_back.f_locals.items()\n",
        "    names = [var_name for var_name, var_val in callers_local_vars if var_val is var]\n",
        "    return names[0] if len(names) != 0 else \"\"\n",
        "\n",
        "def assert_same(x, y, line, dtype):\n",
        "    assert(x.dtype == dtype)\n",
        "    # Tolerances loosened due to https://x.com/danielhanchen/status/1893177157733490920\n",
        "    try: torch.testing.assert_close(x, y, check_stride = True, atol=0.001, rtol=0.001)\n",
        "    except Exception as error:\n",
        "        raise RuntimeError(\n",
        "            f\"Failed allclose at line [{line}]: {NAME(x)}, {NAME(y)}\\n{str(error)}\"\n",
        "        )\n",
        "\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKQ9hdqNOXpe"
      },
      "outputs": [],
      "source": [
        "from bitsandbytes.nn import Linear4bit\n",
        "from transformers.activations import ACT2FN\n",
        "from unsloth.kernels.utils import fast_dequantize\n",
        "from peft.utils.integrations import dequantize_module_weight as peft_dequantize\n",
        "def unsloth_dequantize(weight):\n",
        "    return fast_dequantize(weight.weight, weight.weight.quant_state)\n",
        "\n",
        "def bnb_Linear4bit(hd, m, dtype = torch.float16):\n",
        "    return Linear4bit(\n",
        "        hd, m, bias = None,\n",
        "        compute_dtype       = dtype,\n",
        "        compress_statistics = True,\n",
        "        quant_type          = \"nf4\",\n",
        "    )\n",
        "\n",
        "# [NEW] as at 18th Feb 2025\n",
        "def assert_correct_bnb(weight, dtype):\n",
        "    assert(weight.weight.dtype == torch.uint8)\n",
        "    assert(weight.weight.quant_state.dtype == dtype)\n",
        "    assert(weight.weight.quant_state.absmax.dtype == torch.uint8)\n",
        "    assert(weight.weight.quant_state.code.dtype == torch.float32)\n",
        "    assert(weight.weight.quant_state.offset.dtype == torch.float32)\n",
        "    assert(weight.weight.quant_state.blocksize == 64)\n",
        "    assert(weight.weight.quant_state.state2.absmax.dtype == torch.float32)\n",
        "    assert(weight.weight.quant_state.state2.code.dtype == torch.float32)\n",
        "    assert(weight.weight.quant_state.state2.blocksize == 256)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, hd = 4096, m = 14336, dtype = torch.float16):\n",
        "        super().__init__()\n",
        "        self.gate_proj = bnb_Linear4bit(hd, m, dtype = dtype).to(\"cuda\")\n",
        "        self.up_proj   = bnb_Linear4bit(hd, m, dtype = dtype).to(\"cuda\")\n",
        "        self.down_proj = bnb_Linear4bit(m, hd, dtype = dtype).to(\"cuda\")\n",
        "        # [NEW] as at 18th Feb 2025\n",
        "        self.gate_proj.weight.quant_state.dtype = dtype\n",
        "        self.up_proj  .weight.quant_state.dtype = dtype\n",
        "        self.down_proj.weight.quant_state.dtype = dtype\n",
        "        self.act_fn = ACT2FN[\"silu\"]\n",
        "    def forward(self, x):\n",
        "        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
        "\n",
        "def mlp_forward(X, mlp, fx):\n",
        "    up   = X @ fx(mlp.  up_proj).t()\n",
        "    gate = X @ fx(mlp.gate_proj).t()\n",
        "    h = mlp.act_fn(gate) * up\n",
        "    down = h @ fx(mlp.down_proj).t()\n",
        "    return down\n",
        "\n",
        "def mlp_dequantize(X, mlp, fx):\n",
        "    a = fx(mlp.  up_proj).t(); torch.cuda.synchronize()\n",
        "    b = fx(mlp.gate_proj).t(); torch.cuda.synchronize()\n",
        "    c = fx(mlp.down_proj).t(); torch.cuda.synchronize()\n",
        "    return a, b, c\n",
        "\n",
        "def test_dequantize(dequantize_fx, compile=False):\n",
        "    elapsed = 0\n",
        "    # Note: The latter two won't actually run in bf16 on a T4.\n",
        "    options = [\n",
        "        (2, 3333, 2048,  8192, 3407, torch.float16),\n",
        "        (5,  777, 1024,  4096, 3409, torch.bfloat16),\n",
        "        (3, 2048, 4096, 14336, 3408, torch.bfloat16),\n",
        "    ]\n",
        "    for (bsz, qlen, hd, m, seed, dt) in options:\n",
        "        if not HAS_BFLOAT16 and dt == torch.bfloat16:\n",
        "            dt = torch.float16 # Coerce to float16 for T4 instances\n",
        "        set_seed(seed)\n",
        "        torch.set_default_dtype(torch.float32)\n",
        "        mlp = MLP(hd = hd, m = m, dtype = dt)\n",
        "        if compile:\n",
        "            mlp = torch.compile(mlp)\n",
        "            dequantize_fx = torch.compile(dequantize_fx)\n",
        "        X = torch.randn((bsz, qlen, hd), device = \"cuda\", dtype = dt)\n",
        "        torch.cuda.synchronize()\n",
        "        # Warmup\n",
        "        for _ in range(2):\n",
        "            assert_same( mlp_forward(X, mlp, dequantize_fx), mlp(X), _F(_C()), dt)\n",
        "            # [NEW] as at 18th Feb 2025\n",
        "            assert_correct_bnb(mlp.  up_proj, dt)\n",
        "            assert_correct_bnb(mlp.gate_proj, dt)\n",
        "            assert_correct_bnb(mlp.down_proj, dt)\n",
        "            a, b, c = mlp_dequantize(X, mlp, dequantize_fx)\n",
        "            A, B, C = mlp_dequantize(X, mlp, unsloth_dequantize)\n",
        "            assert_same(a, A, _F(_C()), dt)\n",
        "            assert_same(b, B, _F(_C()), dt)\n",
        "            assert_same(c, C, _F(_C()), dt)\n",
        "\n",
        "        # Benchmarking\n",
        "        torch.cuda.synchronize()\n",
        "        start = time.time()\n",
        "        for _ in range(1000): mlp_dequantize(X, mlp, dequantize_fx)\n",
        "        elapsed += time.time() - start\n",
        "    return elapsed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9EiO1cu2YKB"
      },
      "source": [
        "For example, we can test our implementation via:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OM8q3rDX1XfZ",
        "outputId": "1f079abc-c0dd-4ead-efe9-7096c1181fe6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.58371901512146"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from unsloth.kernels.utils import fast_dequantize\n",
        "def unsloth_dequantize(weight):\n",
        "    return fast_dequantize(weight.weight, weight.weight.quant_state)\n",
        "test_dequantize(unsloth_dequantize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nETwlex22lMN"
      },
      "source": [
        "The elapsed time for our implementation over 1000 trials is 5.38 seconds or so.\n",
        "\n",
        "PEFT also has one, which should be mostly identical to Unsloth's version, albeit slightly slower."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zu5RShLO1h-Y",
        "outputId": "c7613e5b-5347-4df1-9ea1-60646ed9ec5f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.694403648376465"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from peft.utils.integrations import dequantize_module_weight as peft_dequantize\n",
        "test_dequantize(peft_dequantize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lE5pUaSN3JcM"
      },
      "source": [
        "Write your Triton kernel below, and test it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9ThmhbT2GPi"
      },
      "outputs": [],
      "source": [
        "from triton import jit\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "@triton.jit\n",
        "def _your_dequantize_nf4_kernel(w_ptr, absmax_ptr, absmax2_ptr, out_ptr,\n",
        "                                code_ptr, code2_ptr,  # Can we make these constexpr somehow?,\n",
        "                                num_blocks: tl.constexpr,\n",
        "                                num_elements: tl.constexpr,\n",
        "                                n_absmax: tl.constexpr,\n",
        "                                n_absmax2: tl.constexpr,\n",
        "                                n_out: tl.constexpr,\n",
        "                                offset: tl.constexpr,\n",
        "                                kernel_dtype: tl.constexpr,\n",
        "                                blocksize: tl.constexpr,\n",
        "                                blocksize2: tl.constexpr,):\n",
        "    # Contiguous Stride Solution\n",
        "    # We know that absmax, absmax2, and w are contiguous\n",
        "    # Therefore, for each program_id we can process slices of `absmax`, provided they all share the same absmax2.\n",
        "    # If this is insufficient we can generalize this to slicing over absmax2.\n",
        "    first_block = tl.program_id(0) * num_blocks # What is the first absmax block we are processing\n",
        "    last_block = first_block + (num_blocks-1)\n",
        "    # Assert all absmax1 blocks share an absmax2 block\n",
        "    block2 = first_block // blocksize2\n",
        "    last_block2 = last_block // blocksize2\n",
        "    tl.device_assert(block2 == last_block2)\n",
        "    absmax2 = tl.load(absmax2_ptr + block2, mask=block2 < n_absmax2)\n",
        "\n",
        "    # Read the absmax blocks we want\n",
        "    absmax_read_range = first_block + tl.arange(0, num_blocks)\n",
        "    absmax_ix = tl.load(absmax_ptr + absmax_read_range, mask=absmax_read_range < n_absmax).cast(tl.uint16) # Must upcast due to https://github.com/triton-lang/triton/issues/6043\n",
        "    absmax_codes = tl.load(code2_ptr + absmax_ix, mask = absmax_ix < 256)\n",
        "    offsetted_absmax = tl.fma(absmax_codes, absmax2, offset)\n",
        "\n",
        "    # Load the slice of `w_ptr` we are working with\n",
        "    first_element = first_block * blocksize\n",
        "    w_offset = first_element // 2\n",
        "    w_range = w_offset + tl.arange(0, num_elements // 2)\n",
        "    n_w = n_out // 2\n",
        "    w = tl.load(w_ptr + w_range, mask=w_range < n_w)\n",
        "    unpacked_w = tl.interleave(w >> 4, w & 0xF).cast(tl.uint16)\n",
        "\n",
        "    #`gather` is not supported in triton 3.1.0 or 3.2.0: https://github.com/triton-lang/triton/issues/5826\n",
        "    output = tl.load(code_ptr + unpacked_w, mask=unpacked_w < 16).reshape((num_blocks, blocksize))\n",
        "    offsetted_absmax = offsetted_absmax.expand_dims(-1)\n",
        "    write_out = output * offsetted_absmax\n",
        "    write_out = write_out.reshape((num_elements,))\n",
        "    o_offset = first_element\n",
        "    o_range = o_offset + tl.arange(0, num_elements)\n",
        "    tl.store(out_ptr + o_range, write_out, mask=o_range<n_out, cache_modifier=\".cs\") # We don't need the output in cache, it's never reused\n",
        "    return\n",
        "\n",
        "TORCH_TO_TRITON_DTYPE = {\n",
        "    torch.float16  : tl.float16,\n",
        "    torch.bfloat16 : tl.bfloat16,\n",
        "    torch.float32  : tl.float32\n",
        "}\n",
        "\n",
        "\n",
        "def _your_dequantize_nf4(weight, quant_state):\n",
        "    ### SETUP TRITON LAUNCH HERE\n",
        "    kernel_dtype = quant_state.dtype\n",
        "    if not HAS_BFLOAT16 and quant_state.dtype == torch.bfloat16:\n",
        "        kernel_dtype = torch.float16 # Coerce to float16 for T4 instance\n",
        "    out = torch.empty(quant_state.shape,\n",
        "                      dtype=kernel_dtype,\n",
        "                      device=weight.device,\n",
        "                      requires_grad = False)\n",
        "    is_transposed = weight.shape[0] == 1\n",
        "    n_out = out.numel()\n",
        "    n_absmax = quant_state.absmax.numel()\n",
        "    n_absmax2 = quant_state.state2.absmax.numel()\n",
        "\n",
        "    ov = out.view(-1)\n",
        "    grid = (n_absmax // 64,)\n",
        "\n",
        "    num_blocks = n_absmax // grid[0]\n",
        "    compiled_kernel = _your_dequantize_nf4_kernel[grid](weight, quant_state.absmax,\n",
        "                                      quant_state.state2.absmax, ov,\n",
        "                                      quant_state.code,\n",
        "                                      quant_state.state2.code,\n",
        "                                      num_blocks=num_blocks,\n",
        "                                      num_elements=num_blocks * quant_state.blocksize,\n",
        "                                      n_absmax=n_absmax,\n",
        "                                      n_absmax2=n_absmax2,\n",
        "                                      n_out=n_out,\n",
        "                                      offset=quant_state.offset.item(),\n",
        "                                      kernel_dtype=TORCH_TO_TRITON_DTYPE[kernel_dtype],\n",
        "                                      blocksize=quant_state.blocksize,\n",
        "                                      blocksize2=quant_state.state2.blocksize)\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    out = ov.view(out.shape)\n",
        "    if is_transposed:\n",
        "        return out.transpose()\n",
        "    else:\n",
        "        return out\n",
        "\n",
        "def your_dequantize_nf4(weight):\n",
        "    return _your_dequantize_nf4(weight.weight.data, weight.weight.quant_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hodw-RS4ZmG8"
      },
      "outputs": [],
      "source": [
        "set_seed(3407)\n",
        "a = bnb_Linear4bit(2048, 8192, dtype = torch.float16).to(\"cuda\")\n",
        "a.weight.quant_state.dtype = torch.float16\n",
        "\n",
        "expected = unsloth_dequantize(a)\n",
        "actual = your_dequantize_nf4(a)\n",
        "\n",
        "torch.testing.assert_close(expected, actual, atol=0.001, rtol=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrObV0zMi83M"
      },
      "source": [
        "Note that above, we see a slight difference in our dequantization. This could possibly be a bug, or possibly an issue with CUDA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvvEm6ZH35fB",
        "outputId": "a4c10f5b-e30b-4e9a-ddc5-5fbe86b2ac8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Can we use BFLOAT16: False\n",
            "Triton kernel time: 4.000486612319946\n",
            "Reference unsloth kernel time: 4.635912656784058\n",
            "1.1588371880828812\n",
            "Triton kernel time: 3.976804733276367\n",
            "Reference unsloth kernel time: 4.66949462890625\n",
            "1.1741825264473564\n",
            "Triton kernel time: 3.968602180480957\n",
            "Reference unsloth kernel time: 4.737403392791748\n",
            "1.193720906593268\n",
            "Triton kernel time: 3.9665496349334717\n",
            "Reference unsloth kernel time: 4.81851053237915\n",
            "1.214786395194061\n",
            "Triton kernel time: 3.9662768840789795\n",
            "Reference unsloth kernel time: 4.902519464492798\n",
            "1.2360507366926365\n",
            "Average runtime ratio: 1.1955155506020405\n"
          ]
        }
      ],
      "source": [
        "print(\"Can we use BFLOAT16:\", HAS_BFLOAT16)\n",
        "# TEST IT BELOW:\n",
        "RUNS = 5\n",
        "bench = []\n",
        "for _ in range(RUNS):\n",
        "    torch.cuda.synchronize()\n",
        "    torch.cuda.empty_cache()\n",
        "    dequant_time = test_dequantize(your_dequantize_nf4)\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    torch.cuda.empty_cache()\n",
        "    reference_time = test_dequantize(unsloth_dequantize)\n",
        "\n",
        "    print(\"Triton kernel time:\", dequant_time)\n",
        "    print(\"Reference unsloth kernel time:\", reference_time)\n",
        "    ### CALCULATE SPEEDUP (hopefully 1.15x faster or more)\n",
        "    # Somehow, it is!\n",
        "    # The tolerances are really loose (1e-3 rtol and atol)\n",
        "    ratio = reference_time / dequant_time\n",
        "    bench.append(ratio)\n",
        "    print(ratio)\n",
        "print(\"Average runtime ratio:\", sum(bench)/len(bench))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0oFBmQl222R"
      },
      "source": [
        "**NOTE:** The result above shows the kernel's performance on a T4 (where we are only testing float16), but is significantly slower on an L4 GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZR1bfMJP5D9r",
        "outputId": "e20f69ed-694e-4140-b2ff-06ef0be3ce26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Triton kernel time: 5.698766708374023\n",
            "Reference unsloth kernel time: 5.064429759979248\n",
            "0.888688731991318\n",
            "Triton kernel time: 5.911512136459351\n",
            "Reference unsloth kernel time: 5.176299333572388\n",
            "0.875630331814338\n",
            "Triton kernel time: 5.882539987564087\n",
            "Reference unsloth kernel time: 5.290977716445923\n",
            "0.8994376115812643\n",
            "Triton kernel time: 5.869607210159302\n",
            "Reference unsloth kernel time: 5.25044059753418\n",
            "0.8945131095734228\n",
            "Triton kernel time: 5.866735935211182\n",
            "Reference unsloth kernel time: 5.166455030441284\n",
            "0.8806353460419231\n",
            "Average runtime ratio: 0.8877810262004532\n"
          ]
        }
      ],
      "source": [
        "#@title Test with compiled MLP\n",
        "\n",
        "RUNS = 5\n",
        "bench = []\n",
        "for _ in range(RUNS):\n",
        "    torch.cuda.synchronize()\n",
        "    torch.cuda.empty_cache()\n",
        "    dequant_time = test_dequantize(your_dequantize_nf4, compile=True)\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    torch.cuda.empty_cache()\n",
        "    reference_time = test_dequantize(unsloth_dequantize, compile=True)\n",
        "\n",
        "    print(\"Triton kernel time:\", dequant_time)\n",
        "    print(\"Reference unsloth kernel time:\", reference_time)\n",
        "    ### CALCULATE SPEEDUP (hopefully 1.15x faster or more)\n",
        "    # The triton kernel isn't compile friendly. We lose time on compilation?\n",
        "    ratio = reference_time / dequant_time\n",
        "    bench.append(ratio)\n",
        "    print(ratio)\n",
        "\n",
        "print(\"Average runtime ratio:\", sum(bench)/len(bench))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVqVEkswYfUi"
      },
      "source": [
        "Misc. Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAPJ2Iy7shXh"
      },
      "source": [
        "### Kernel Parameter Sweep\n",
        "\n",
        "With GPU code, kernel launch parameters can dramatically affect performance. Selecting good parameters can be tricky and input shape dependent.\n",
        "\n",
        "In total we have 3 knobs, `(size, num_warps, num_stages)` in which to tune our parameters in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krd2reoaNOKE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dbf8b8a-6576-409a-99c4-f4b8ac59afb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 1, 1) - Average runtime ratio: 0.4117947205358429\n",
            "(1, 1, 2) - Average runtime ratio: 0.41324931640489404\n",
            "(1, 1, 3) - Average runtime ratio: 0.4132550341887066\n",
            "(1, 1, 4) - Average runtime ratio: 0.41390663854701126\n",
            "(1, 1, 8) - Average runtime ratio: 0.41316792619909765\n",
            "(1, 2, 1) - Average runtime ratio: 0.4148985525419137\n",
            "(1, 2, 2) - Average runtime ratio: 0.4124578548290544\n",
            "(1, 2, 3) - Average runtime ratio: 0.41335346684326457\n",
            "(1, 2, 4) - Average runtime ratio: 0.4168257273399646\n",
            "(1, 2, 8) - Average runtime ratio: 0.41670801757226655\n",
            "(1, 4, 1) - Average runtime ratio: 0.41539329132416014\n",
            "(1, 4, 2) - Average runtime ratio: 0.41341721693988975\n",
            "(1, 4, 3) - Average runtime ratio: 0.41152206717844486\n",
            "(1, 4, 4) - Average runtime ratio: 0.4125461569600087\n",
            "(1, 4, 8) - Average runtime ratio: 0.4147916797269728\n",
            "(2, 1, 1) - Average runtime ratio: 0.6808346438779074\n",
            "(2, 1, 2) - Average runtime ratio: 0.6855004614324384\n",
            "(2, 1, 3) - Average runtime ratio: 0.6844380209006784\n",
            "(2, 1, 4) - Average runtime ratio: 0.6843765388421521\n",
            "(2, 1, 8) - Average runtime ratio: 0.6829999838728091\n",
            "(2, 2, 1) - Average runtime ratio: 0.6839894822861657\n",
            "(2, 2, 2) - Average runtime ratio: 0.6819549057533918\n",
            "(2, 2, 3) - Average runtime ratio: 0.6826626486367781\n",
            "(2, 2, 4) - Average runtime ratio: 0.6836918665292285\n",
            "(2, 2, 8) - Average runtime ratio: 0.6812571087973753\n",
            "(2, 4, 1) - Average runtime ratio: 0.6827341848903173\n",
            "(2, 4, 2) - Average runtime ratio: 0.6857441670910897\n",
            "(2, 4, 3) - Average runtime ratio: 0.6833828631975009\n",
            "(2, 4, 4) - Average runtime ratio: 0.6825769907716985\n",
            "(2, 4, 8) - Average runtime ratio: 0.6827121259357973\n",
            "(4, 1, 1) - Average runtime ratio: 1.1992514109593941\n",
            "(4, 1, 2) - Average runtime ratio: 1.1980725953575793\n",
            "(4, 1, 3) - Average runtime ratio: 1.207907542784228\n",
            "(4, 1, 4) - Average runtime ratio: 1.2068388201315825\n",
            "(4, 1, 8) - Average runtime ratio: 1.2025650818487537\n",
            "(4, 2, 1) - Average runtime ratio: 1.2053102890624938\n",
            "(4, 2, 2) - Average runtime ratio: 1.2052522881868373\n",
            "(4, 2, 3) - Average runtime ratio: 1.2091107912940353\n",
            "(4, 2, 4) - Average runtime ratio: 1.1967810511376362\n",
            "(4, 2, 8) - Average runtime ratio: 1.1903911259203488\n",
            "(4, 4, 1) - Average runtime ratio: 1.2072730250528707\n",
            "(4, 4, 2) - Average runtime ratio: 1.1966830828046138\n",
            "(4, 4, 3) - Average runtime ratio: 1.203836697669927\n",
            "(4, 4, 4) - Average runtime ratio: 1.2066730590133916\n",
            "(4, 4, 8) - Average runtime ratio: 1.2024305260521901\n",
            "(8, 1, 1) - Average runtime ratio: 1.3524056234710662\n",
            "(8, 1, 2) - Average runtime ratio: 1.3615781784179302\n",
            "(8, 1, 3) - Average runtime ratio: 1.369875470350962\n",
            "(8, 1, 4) - Average runtime ratio: 1.3685391415518031\n",
            "(8, 1, 8) - Average runtime ratio: 1.3660234974141587\n",
            "(8, 2, 1) - Average runtime ratio: 1.3530095246658207\n",
            "(8, 2, 2) - Average runtime ratio: 1.3595495444233923\n",
            "(8, 2, 3) - Average runtime ratio: 1.356241899523972\n",
            "(8, 2, 4) - Average runtime ratio: 1.3483749229279047\n",
            "(8, 2, 8) - Average runtime ratio: 1.354235021820066\n",
            "(8, 4, 1) - Average runtime ratio: 1.3541166683522006\n",
            "(8, 4, 2) - Average runtime ratio: 1.3707141794563604\n",
            "(8, 4, 3) - Average runtime ratio: 1.3527777322126844\n",
            "(8, 4, 4) - Average runtime ratio: 1.3639086215874716\n",
            "(8, 4, 8) - Average runtime ratio: 1.3644865739355385\n",
            "(16, 1, 1) - Average runtime ratio: 1.3257968680396754\n",
            "(16, 1, 2) - Average runtime ratio: 1.3515452317410062\n",
            "(16, 1, 3) - Average runtime ratio: 1.3290657124952168\n",
            "(16, 1, 4) - Average runtime ratio: 1.3274607310504867\n",
            "(16, 1, 8) - Average runtime ratio: 1.3399210978678429\n",
            "(16, 2, 1) - Average runtime ratio: 1.3193097525541388\n",
            "(16, 2, 2) - Average runtime ratio: 1.3348377276562764\n",
            "(16, 2, 3) - Average runtime ratio: 1.3367434210771525\n",
            "(16, 2, 4) - Average runtime ratio: 1.337396794179707\n",
            "(16, 2, 8) - Average runtime ratio: 1.3259524456857559\n",
            "(16, 4, 1) - Average runtime ratio: 1.335815126131654\n",
            "(16, 4, 2) - Average runtime ratio: 1.3232672913177748\n",
            "(16, 4, 3) - Average runtime ratio: 1.3344298525255838\n",
            "(16, 4, 4) - Average runtime ratio: 1.3290692692210273\n",
            "(16, 4, 8) - Average runtime ratio: 1.329023124883721\n",
            "(32, 1, 1) - Average runtime ratio: 1.331738097066593\n",
            "(32, 1, 2) - Average runtime ratio: 1.3248137598860603\n",
            "(32, 1, 3) - Average runtime ratio: 1.321665636299676\n",
            "(32, 1, 4) - Average runtime ratio: 1.3125047187981451\n",
            "(32, 1, 8) - Average runtime ratio: 1.3142470618449804\n",
            "(32, 2, 1) - Average runtime ratio: 1.3146587729618526\n",
            "(32, 2, 2) - Average runtime ratio: 1.314524814888609\n",
            "(32, 2, 3) - Average runtime ratio: 1.3222790570710263\n",
            "(32, 2, 4) - Average runtime ratio: 1.3134019723651438\n",
            "(32, 2, 8) - Average runtime ratio: 1.3157858227588553\n",
            "(32, 4, 1) - Average runtime ratio: 1.3281492076934656\n",
            "(32, 4, 2) - Average runtime ratio: 1.3103921225838888\n",
            "(32, 4, 3) - Average runtime ratio: 1.3161953262193151\n",
            "(32, 4, 4) - Average runtime ratio: 1.3191840992974184\n",
            "(32, 4, 8) - Average runtime ratio: 1.3109506472069254\n",
            "(64, 1, 1) - Average runtime ratio: 1.2868977329838458\n",
            "(64, 1, 2) - Average runtime ratio: 1.297567959827144\n",
            "(64, 1, 3) - Average runtime ratio: 1.2736948078259027\n",
            "(64, 1, 4) - Average runtime ratio: 1.278285084272556\n",
            "(64, 1, 8) - Average runtime ratio: 1.2938319070403412\n",
            "(64, 2, 1) - Average runtime ratio: 1.2822263315398958\n",
            "(64, 2, 2) - Average runtime ratio: 1.2755111926568838\n",
            "(64, 2, 3) - Average runtime ratio: 1.2964006435232112\n",
            "(64, 2, 4) - Average runtime ratio: 1.281055452179543\n",
            "(64, 2, 8) - Average runtime ratio: 1.2772805384930994\n",
            "(64, 4, 1) - Average runtime ratio: 1.2951623551217295\n",
            "(64, 4, 2) - Average runtime ratio: 1.278209140756899\n",
            "(64, 4, 3) - Average runtime ratio: 1.267737729626589\n",
            "(64, 4, 4) - Average runtime ratio: 1.3000283291791332\n",
            "(64, 4, 8) - Average runtime ratio: 1.2812470025753484\n",
            "(128, 1, 1) - Average runtime ratio: 1.2864181572651177\n",
            "(128, 1, 2) - Average runtime ratio: 1.292715335798243\n",
            "(128, 1, 3) - Average runtime ratio: 1.2797760822113067\n",
            "(128, 1, 4) - Average runtime ratio: 1.28070325147198\n",
            "(128, 1, 8) - Average runtime ratio: 1.2966861995443324\n",
            "(128, 2, 1) - Average runtime ratio: 1.2882654558489148\n",
            "(128, 2, 2) - Average runtime ratio: 1.283724535127226\n",
            "(128, 2, 3) - Average runtime ratio: 1.2901539383089058\n",
            "(128, 2, 4) - Average runtime ratio: 1.28976253878494\n",
            "(128, 2, 8) - Average runtime ratio: 1.2920236649044168\n",
            "(128, 4, 1) - Average runtime ratio: 1.2876498059869699\n",
            "(128, 4, 2) - Average runtime ratio: 1.2816289474848102\n",
            "(128, 4, 3) - Average runtime ratio: 1.2849724226025365\n",
            "(128, 4, 4) - Average runtime ratio: 1.2937218379896764\n",
            "(128, 4, 8) - Average runtime ratio: 1.2949707834665871\n"
          ]
        }
      ],
      "source": [
        "#  Parameter Sweep\n",
        "RUNS = 1\n",
        "\n",
        "SIZE_DENOM = [1, 2, 4, 8, 16, 32, 64, 128]\n",
        "\n",
        "WARPS = [1,2,4]\n",
        "\n",
        "# -1 = programatically guess the optimal.\n",
        "STAGES = [1, 2, 3, 4, 8]\n",
        "\n",
        "def _your_dequantize_nf4_param(weight, quant_state, size=1, warps=1, stages=1):\n",
        "    ### SETUP TRITON LAUNCH HERE\n",
        "    kernel_dtype = quant_state.dtype\n",
        "    if not HAS_BFLOAT16 and quant_state.dtype == torch.bfloat16:\n",
        "        kernel_dtype = torch.float16 # Coerce to float16 for T4 instance\n",
        "    out = torch.empty(quant_state.shape,\n",
        "                      dtype=kernel_dtype,\n",
        "                      device=weight.device,\n",
        "                      requires_grad = False)\n",
        "    is_transposed = weight.shape[0] == 1\n",
        "    n_out = out.numel()\n",
        "    n_absmax = quant_state.absmax.numel()\n",
        "    n_absmax2 = quant_state.state2.absmax.numel()\n",
        "\n",
        "    ov = out.view(-1)\n",
        "    grid = (n_absmax // size,)\n",
        "\n",
        "    num_blocks = n_absmax // grid[0]\n",
        "    compiled_kernel = _your_dequantize_nf4_kernel[grid](weight, quant_state.absmax,\n",
        "                                      quant_state.state2.absmax, ov,\n",
        "                                      quant_state.code,\n",
        "                                      quant_state.state2.code,\n",
        "                                      num_blocks=num_blocks,\n",
        "                                      num_elements=num_blocks * quant_state.blocksize,\n",
        "                                      n_absmax=n_absmax,\n",
        "                                      n_absmax2=n_absmax2,\n",
        "                                      n_out=n_out,\n",
        "                                      offset=quant_state.offset.item(),\n",
        "                                      kernel_dtype=TORCH_TO_TRITON_DTYPE[kernel_dtype],\n",
        "                                      blocksize=quant_state.blocksize,\n",
        "                                      blocksize2=quant_state.state2.blocksize)\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    out = ov.view(out.shape)\n",
        "    if is_transposed:\n",
        "        return out.transpose()\n",
        "    else:\n",
        "        return out\n",
        "\n",
        "def your_dequantize_nf4_param(weight, size=1, warps=1, stages=1):\n",
        "    return _your_dequantize_nf4_param(weight.weight.data, weight.weight.quant_state, size=size, warps=warps, stages=stages)\n",
        "\n",
        "from functools import partial\n",
        "for sz in SIZE_DENOM:\n",
        "    for warp in WARPS:\n",
        "        for stage in STAGES:\n",
        "            bench = []\n",
        "            parameterized_dequant = partial(your_dequantize_nf4_param, size=sz, warps=warp, stages=stage)\n",
        "            for _ in range(RUNS):\n",
        "                torch.cuda.synchronize()\n",
        "                torch.cuda.empty_cache()\n",
        "                dequant_time = test_dequantize(parameterized_dequant)\n",
        "\n",
        "                torch.cuda.synchronize()\n",
        "                torch.cuda.empty_cache()\n",
        "                reference_time = test_dequantize(unsloth_dequantize)\n",
        "\n",
        "                ### CALCULATE SPEEDUP (hopefully 1.15x faster or more)\n",
        "                # (It's not. It's actually quite subpar. I tried :P)\n",
        "                ratio = reference_time / dequant_time\n",
        "                bench.append(ratio)\n",
        "                print(f\"{(sz, warp, stage)} - Average runtime ratio:\", sum(bench)/len(bench))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the performance of the kernel varies drastically, depending on if we pick good kernel params or not."
      ],
      "metadata": {
        "id": "YIWWuNo_4D8R"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyPvqmaAHwz31sNtzTep8PHk",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}