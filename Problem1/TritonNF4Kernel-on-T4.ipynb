{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/byi8220/unsloth-puzzles/blob/main/Problem1/TritonNF4Kernel-on-T4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrBg_fkWSKoe"
      },
      "source": [
        "# Unsloth Problem 1 - Convert nf4 to Triton\n",
        "\n",
        "Run on a Tesla T4 colab instance\n",
        "\n",
        "(Note: Tesla T4 does not support `bfloat16`. Since we must use a T4, we can only do regular `float16`.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoE2DGRZG2Ng"
      },
      "source": [
        "## Problem Statement\n",
        "---\n",
        "---\n",
        "---\n",
        "<a name=\"NF4\"></a>\n",
        "## A) Convert `nf4` to Triton. [Difficulty: Hard] [Max points: 14]\n",
        "\n",
        "1. Goal: Convert a `nf4` quantized tensor into `fp16` or `bf16` into a *single* Triton kernel The double dequant of the `absmax` and weight forming must be done in 1 Triton kernel. Must work on Tesla T4.\n",
        "2. Must be faster than Unsloth's `fast_dequantize` by 1.15x or more, and not use large intermediate memory buffers.\n",
        "3. Must not use `torch.compile`, but can use `trace.enabled` to help on writing Triton kernels.\n",
        "4. Good material: [Unsloth `fast_dequantize` function](https://github.com/unslothai/unsloth/blob/main/unsloth/kernels/utils.py#L128), also [bitsandbytes `dequantize_blockwise`](https://github.com/bitsandbytes-foundation/bitsandbytes/blob/86b6c37a8ad448230cedb60753f63150b603a112/bitsandbytes/functional.py#L958)\n",
        "5. Use `test_dequantize_function` to test your implementation.\n",
        "6. No CUDA allowed. Custom CUDA inside of the Triton is allowed.\n",
        "7. Watch Tim's videos on Youtube: [8-bit Optimizers](https://www.youtube.com/watch?v=2ETNONas068)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "F_rx9FYMOc2T"
      },
      "outputs": [],
      "source": [
        "# Code to install Unsloth, Triton, Torch etc\n",
        "%%capture\n",
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl\n",
        "!pip install triton==3.1.0 # (https://github.com/unslothai/unsloth/issues/1604)\n",
        "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "\n",
        "!pip install --no-deps unsloth==2025.3.4 # Stick to stable version\n",
        "!pip install --no-deps unsloth_zoo==2025.3.4 # Stick to stable version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EI_d4FLkR51i",
        "outputId": "8573d01e-50ec-4915-ce07-c81e05b896f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:xformers:WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
            "    PyTorch 2.5.1+cu121 with CUDA 1201 (you have 2.6.0+cu124)\n",
            "    Python  3.11.11 (you have 3.11.11)\n",
            "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
            "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
            "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ],
      "source": [
        "# Unsloth yells at me to import it before transformers.\n",
        "import unsloth\n",
        "\n",
        "# Helpful functions used through the entire notebook\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import set_seed\n",
        "import time\n",
        "import inspect\n",
        "import os\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "HAS_BFLOAT16 = (major_version >= 8)\n",
        "from inspect import currentframe as _C, getframeinfo\n",
        "_F = lambda c: getframeinfo(c).lineno # Gets line number\n",
        "WARN = lambda x: print(f\"\\033[31m{x}\\033[0m\") # Red colored warnings\n",
        "\n",
        "# https://stackoverflow.com/questions/18425225/getting-the-name-of-a-variable-as-a-string\n",
        "def NAME(var):\n",
        "    callers_local_vars = inspect.currentframe().f_back.f_locals.items()\n",
        "    names = [var_name for var_name, var_val in callers_local_vars if var_val is var]\n",
        "    return names[0] if len(names) != 0 else \"\"\n",
        "\n",
        "def assert_same(x, y, line, dtype):\n",
        "    assert(x.dtype == dtype)\n",
        "    # Tolerances loosened due to https://x.com/danielhanchen/status/1893177157733490920\n",
        "    try: torch.testing.assert_close(x, y, check_stride = True, atol=0.001, rtol=0.001)\n",
        "    except Exception as error:\n",
        "        raise RuntimeError(\n",
        "            f\"Failed allclose at line [{line}]: {NAME(x)}, {NAME(y)}\\n{str(error)}\"\n",
        "        )\n",
        "\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WKQ9hdqNOXpe"
      },
      "outputs": [],
      "source": [
        "from bitsandbytes.nn import Linear4bit\n",
        "from transformers.activations import ACT2FN\n",
        "from unsloth.kernels.utils import fast_dequantize\n",
        "from peft.utils.integrations import dequantize_module_weight as peft_dequantize\n",
        "def unsloth_dequantize(weight):\n",
        "    return fast_dequantize(weight.weight, weight.weight.quant_state)\n",
        "\n",
        "def bnb_Linear4bit(hd, m, dtype = torch.float16):\n",
        "    return Linear4bit(\n",
        "        hd, m, bias = None,\n",
        "        compute_dtype       = dtype,\n",
        "        compress_statistics = True,\n",
        "        quant_type          = \"nf4\",\n",
        "    )\n",
        "\n",
        "# [NEW] as at 18th Feb 2025\n",
        "def assert_correct_bnb(weight, dtype):\n",
        "    assert(weight.weight.dtype == torch.uint8)\n",
        "    assert(weight.weight.quant_state.dtype == dtype)\n",
        "    assert(weight.weight.quant_state.absmax.dtype == torch.uint8)\n",
        "    assert(weight.weight.quant_state.code.dtype == torch.float32)\n",
        "    assert(weight.weight.quant_state.offset.dtype == torch.float32)\n",
        "    assert(weight.weight.quant_state.blocksize == 64)\n",
        "    assert(weight.weight.quant_state.state2.absmax.dtype == torch.float32)\n",
        "    assert(weight.weight.quant_state.state2.code.dtype == torch.float32)\n",
        "    assert(weight.weight.quant_state.state2.blocksize == 256)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, hd = 4096, m = 14336, dtype = torch.float16):\n",
        "        super().__init__()\n",
        "        self.gate_proj = bnb_Linear4bit(hd, m, dtype = dtype).to(\"cuda\")\n",
        "        self.up_proj   = bnb_Linear4bit(hd, m, dtype = dtype).to(\"cuda\")\n",
        "        self.down_proj = bnb_Linear4bit(m, hd, dtype = dtype).to(\"cuda\")\n",
        "        # [NEW] as at 18th Feb 2025\n",
        "        self.gate_proj.weight.quant_state.dtype = dtype\n",
        "        self.up_proj  .weight.quant_state.dtype = dtype\n",
        "        self.down_proj.weight.quant_state.dtype = dtype\n",
        "        self.act_fn = ACT2FN[\"silu\"]\n",
        "    def forward(self, x):\n",
        "        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
        "\n",
        "def mlp_forward(X, mlp, fx):\n",
        "    up   = X @ fx(mlp.  up_proj).t()\n",
        "    gate = X @ fx(mlp.gate_proj).t()\n",
        "    h = mlp.act_fn(gate) * up\n",
        "    down = h @ fx(mlp.down_proj).t()\n",
        "    return down\n",
        "\n",
        "def mlp_dequantize(X, mlp, fx):\n",
        "    a = fx(mlp.  up_proj).t(); torch.cuda.synchronize()\n",
        "    b = fx(mlp.gate_proj).t(); torch.cuda.synchronize()\n",
        "    c = fx(mlp.down_proj).t(); torch.cuda.synchronize()\n",
        "    return a, b, c\n",
        "\n",
        "def test_dequantize(dequantize_fx, compile=False):\n",
        "    elapsed = 0\n",
        "    # Note: The latter two won't actually run in bf16 on a T4.\n",
        "    options = [\n",
        "        (2, 3333, 2048,  8192, 3407, torch.float16),\n",
        "        (5,  777, 1024,  4096, 3409, torch.bfloat16),\n",
        "        (3, 2048, 4096, 14336, 3408, torch.bfloat16),\n",
        "    ]\n",
        "    for (bsz, qlen, hd, m, seed, dt) in options:\n",
        "        if not HAS_BFLOAT16 and dt == torch.bfloat16:\n",
        "            dt = torch.float16 # Coerce to float16 for T4 instances\n",
        "        set_seed(seed)\n",
        "        torch.set_default_dtype(torch.float32)\n",
        "        mlp = MLP(hd = hd, m = m, dtype = dt)\n",
        "        if compile:\n",
        "            mlp = torch.compile(mlp)\n",
        "            dequantize_fx = torch.compile(dequantize_fx)\n",
        "        X = torch.randn((bsz, qlen, hd), device = \"cuda\", dtype = dt)\n",
        "        torch.cuda.synchronize()\n",
        "        # Warmup\n",
        "        for _ in range(2):\n",
        "            assert_same( mlp_forward(X, mlp, dequantize_fx), mlp(X), _F(_C()), dt)\n",
        "            # [NEW] as at 18th Feb 2025\n",
        "            assert_correct_bnb(mlp.  up_proj, dt)\n",
        "            assert_correct_bnb(mlp.gate_proj, dt)\n",
        "            assert_correct_bnb(mlp.down_proj, dt)\n",
        "            a, b, c = mlp_dequantize(X, mlp, dequantize_fx)\n",
        "            A, B, C = mlp_dequantize(X, mlp, unsloth_dequantize)\n",
        "            assert_same(a, A, _F(_C()), dt)\n",
        "            assert_same(b, B, _F(_C()), dt)\n",
        "            assert_same(c, C, _F(_C()), dt)\n",
        "\n",
        "        # Benchmarking\n",
        "        torch.cuda.synchronize()\n",
        "        start = time.time()\n",
        "        for _ in range(1000): mlp_dequantize(X, mlp, dequantize_fx)\n",
        "        elapsed += time.time() - start\n",
        "    return elapsed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9EiO1cu2YKB"
      },
      "source": [
        "For example, we can test our implementation via:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OM8q3rDX1XfZ",
        "outputId": "f75b959e-3f99-476e-da02-90d641c9b530"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.571231126785278"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from unsloth.kernels.utils import fast_dequantize\n",
        "def unsloth_dequantize(weight):\n",
        "    return fast_dequantize(weight.weight, weight.weight.quant_state)\n",
        "test_dequantize(unsloth_dequantize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nETwlex22lMN"
      },
      "source": [
        "The elapsed time for our implementation over 1000 trials is 5.38 seconds or so.\n",
        "\n",
        "PEFT also has one, which should be mostly identical to Unsloth's version, albeit slightly slower."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zu5RShLO1h-Y",
        "outputId": "cf7f5765-576e-41b7-fbba-a2f852b0f069"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.684786796569824"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from peft.utils.integrations import dequantize_module_weight as peft_dequantize\n",
        "test_dequantize(peft_dequantize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lE5pUaSN3JcM"
      },
      "source": [
        "Write your Triton kernel below, and test it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "P9ThmhbT2GPi"
      },
      "outputs": [],
      "source": [
        "from triton import jit\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "@triton.jit\n",
        "def _your_dequantize_nf4_kernel(w_ptr, absmax_ptr, absmax2_ptr, out_ptr,\n",
        "                                code_ptr, code2_ptr,  # Can we make these constexpr somehow?,\n",
        "                                num_blocks: tl.constexpr,\n",
        "                                num_elements: tl.constexpr,\n",
        "                                n_absmax: tl.constexpr,\n",
        "                                n_absmax2: tl.constexpr,\n",
        "                                n_out: tl.constexpr,\n",
        "                                offset: tl.constexpr,\n",
        "                                kernel_dtype: tl.constexpr,\n",
        "                                blocksize: tl.constexpr,\n",
        "                                blocksize2: tl.constexpr,):\n",
        "    # Contiguous Stride Solution\n",
        "    # We know that absmax, absmax2, and w are contiguous\n",
        "    # Therefore, for each program_id we can process slices of `absmax`, provided they all share the same absmax2.\n",
        "    # If this is insufficient we can generalize this to slicing over absmax2.\n",
        "    first_block = tl.program_id(0) * num_blocks # What is the first absmax block we are processing\n",
        "    last_block = first_block + (num_blocks-1)\n",
        "    # Assert all absmax1 blocks share an absmax2 block\n",
        "    block2 = first_block // blocksize2\n",
        "    last_block2 = last_block // blocksize2\n",
        "    tl.device_assert(block2 == last_block2)\n",
        "    absmax2 = tl.load(absmax2_ptr + block2, mask=block2 < n_absmax2)\n",
        "\n",
        "    # Read the absmax blocks we want\n",
        "    absmax_read_range = first_block + tl.arange(0, num_blocks)\n",
        "    absmax_ix = tl.load(absmax_ptr + absmax_read_range, mask=absmax_read_range < n_absmax).cast(tl.uint16) # Must upcast due to https://github.com/triton-lang/triton/issues/6043\n",
        "    absmax_codes = tl.load(code2_ptr + absmax_ix, mask = absmax_ix < 256)\n",
        "    offsetted_absmax = tl.fma(absmax_codes, absmax2, offset)\n",
        "\n",
        "    # Load the slice of `w_ptr` we are working with\n",
        "    first_element = first_block * blocksize\n",
        "    w_offset = first_element // 2\n",
        "    w_range = w_offset + tl.arange(0, num_elements // 2)\n",
        "    n_w = n_out // 2\n",
        "    w = tl.load(w_ptr + w_range, mask=w_range < n_w)\n",
        "    unpacked_w = tl.interleave(w >> 4, w & 0xF).cast(tl.uint16)\n",
        "\n",
        "    #`gather` is not supported in triton 3.1.0 or 3.2.0: https://github.com/triton-lang/triton/issues/5826\n",
        "    output = tl.load(code_ptr + unpacked_w, mask=unpacked_w < 16).reshape((num_blocks, blocksize))\n",
        "    offsetted_absmax = offsetted_absmax.expand_dims(-1)\n",
        "    write_out = output * offsetted_absmax\n",
        "    write_out = write_out.reshape((num_elements,))\n",
        "    o_offset = first_element\n",
        "    o_range = o_offset + tl.arange(0, num_elements)\n",
        "    tl.store(out_ptr + o_range, write_out, mask=o_range<n_out, cache_modifier=\".cs\") # We don't need the output in cache, it's never reused\n",
        "    return\n",
        "\n",
        "TORCH_TO_TRITON_DTYPE = {\n",
        "    torch.float16  : tl.float16,\n",
        "    torch.bfloat16 : tl.bfloat16,\n",
        "    torch.float32  : tl.float32\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _your_dequantize_nf4(weight, quant_state):\n",
        "    ### SETUP TRITON LAUNCH HERE\n",
        "    kernel_dtype = quant_state.dtype\n",
        "    if not HAS_BFLOAT16 and quant_state.dtype == torch.bfloat16:\n",
        "        kernel_dtype = torch.float16 # Coerce to float16 for T4 instance\n",
        "    out = torch.empty(quant_state.shape,\n",
        "                      dtype=kernel_dtype,\n",
        "                      device=weight.device,\n",
        "                      requires_grad = False)\n",
        "    is_transposed = weight.shape[0] == 1\n",
        "    n_out = out.numel()\n",
        "    n_absmax = quant_state.absmax.numel()\n",
        "    n_absmax2 = quant_state.state2.absmax.numel()\n",
        "\n",
        "    ov = out.view(-1)\n",
        "    grid = (n_absmax // 64,)\n",
        "\n",
        "    num_blocks = n_absmax // grid[0]\n",
        "    compiled_kernel = _your_dequantize_nf4_kernel[grid](weight, quant_state.absmax,\n",
        "                                      quant_state.state2.absmax, ov,\n",
        "                                      quant_state.code,\n",
        "                                      quant_state.state2.code,\n",
        "                                      num_blocks=num_blocks,\n",
        "                                      num_elements=num_blocks * quant_state.blocksize,\n",
        "                                      n_absmax=n_absmax,\n",
        "                                      n_absmax2=n_absmax2,\n",
        "                                      n_out=n_out,\n",
        "                                      offset=quant_state.offset.item(),\n",
        "                                      kernel_dtype=TORCH_TO_TRITON_DTYPE[kernel_dtype],\n",
        "                                      blocksize=quant_state.blocksize,\n",
        "                                      blocksize2=quant_state.state2.blocksize)\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    out = ov.view(out.shape)\n",
        "    if is_transposed:\n",
        "        return out.transpose()\n",
        "    else:\n",
        "        return out\n",
        "\n",
        "def your_dequantize_nf4(weight):\n",
        "    return _your_dequantize_nf4(weight.weight.data, weight.weight.quant_state)"
      ],
      "metadata": {
        "id": "XmgxNiLtSMHv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do a basic sanity test"
      ],
      "metadata": {
        "id": "5ztjwzG-SFf2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Hodw-RS4ZmG8"
      },
      "outputs": [],
      "source": [
        "set_seed(3407)\n",
        "a = bnb_Linear4bit(2048, 8192, dtype = torch.float16).to(\"cuda\")\n",
        "a.weight.quant_state.dtype = torch.float16\n",
        "\n",
        "expected = unsloth_dequantize(a)\n",
        "actual = your_dequantize_nf4(a)\n",
        "\n",
        "torch.testing.assert_close(expected, actual, atol=0.001, rtol=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrObV0zMi83M"
      },
      "source": [
        "Note that above, we see a slight difference in our dequantization. This could possibly be a bug, or possibly an issue with CUDA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvvEm6ZH35fB",
        "outputId": "7c15b738-fcd0-4a76-f5b6-028ab58d49ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Can we use BFLOAT16: False\n",
            "Triton kernel time: 3.967067003250122\n",
            "Reference unsloth kernel time: 4.566044569015503\n",
            "1.1509875092290232\n",
            "Triton kernel time: 3.9681382179260254\n",
            "Reference unsloth kernel time: 4.639986515045166\n",
            "1.1693107095121014\n",
            "Triton kernel time: 3.9513871669769287\n",
            "Reference unsloth kernel time: 4.685423374176025\n",
            "1.1857667133541567\n",
            "Triton kernel time: 3.979325532913208\n",
            "Reference unsloth kernel time: 4.76274037361145\n",
            "1.19687126228266\n",
            "Triton kernel time: 3.9761393070220947\n",
            "Reference unsloth kernel time: 4.834353685379028\n",
            "1.2158411242888991\n",
            "Average runtime ratio: 1.1837554637333683\n"
          ]
        }
      ],
      "source": [
        "print(\"Can we use BFLOAT16:\", HAS_BFLOAT16)\n",
        "# TEST IT BELOW:\n",
        "RUNS = 5\n",
        "bench = []\n",
        "for _ in range(RUNS):\n",
        "    torch.cuda.synchronize()\n",
        "    torch.cuda.empty_cache()\n",
        "    dequant_time = test_dequantize(your_dequantize_nf4)\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    torch.cuda.empty_cache()\n",
        "    reference_time = test_dequantize(unsloth_dequantize)\n",
        "\n",
        "    print(\"Triton kernel time:\", dequant_time)\n",
        "    print(\"Reference unsloth kernel time:\", reference_time)\n",
        "    ### CALCULATE SPEEDUP (hopefully 1.15x faster or more)\n",
        "    # Somehow, it is!\n",
        "    # The tolerances are really loose (1e-3 rtol and atol)\n",
        "    ratio = reference_time / dequant_time\n",
        "    bench.append(ratio)\n",
        "    print(ratio)\n",
        "print(\"Average runtime ratio:\", sum(bench)/len(bench))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0oFBmQl222R"
      },
      "source": [
        "**NOTE:** The result above shows the kernel's performance on a T4 (where we are only testing float16), but is significantly slower on an L4 GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZR1bfMJP5D9r",
        "outputId": "b866ec42-fd64-46c1-8eb5-ebdc919759c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Triton kernel time: 5.565509796142578\n",
            "Reference unsloth kernel time: 4.980707883834839\n",
            "0.8949239272360886\n",
            "Triton kernel time: 5.737541913986206\n",
            "Reference unsloth kernel time: 4.991902828216553\n",
            "0.8700420673264216\n",
            "Triton kernel time: 5.822845935821533\n",
            "Reference unsloth kernel time: 4.917914867401123\n",
            "0.8445895566541836\n",
            "Triton kernel time: 5.7411017417907715\n",
            "Reference unsloth kernel time: 4.892543315887451\n",
            "0.852195891299666\n",
            "Triton kernel time: 5.803293228149414\n",
            "Reference unsloth kernel time: 4.925173282623291\n",
            "0.8486859252145453\n",
            "Average runtime ratio: 0.8620874735461811\n"
          ]
        }
      ],
      "source": [
        "#@title Test with compiled MLP\n",
        "\n",
        "RUNS = 5\n",
        "bench = []\n",
        "for _ in range(RUNS):\n",
        "    torch.cuda.synchronize()\n",
        "    torch.cuda.empty_cache()\n",
        "    dequant_time = test_dequantize(your_dequantize_nf4, compile=True)\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    torch.cuda.empty_cache()\n",
        "    reference_time = test_dequantize(unsloth_dequantize, compile=True)\n",
        "\n",
        "    print(\"Triton kernel time:\", dequant_time)\n",
        "    print(\"Reference unsloth kernel time:\", reference_time)\n",
        "    ### CALCULATE SPEEDUP (hopefully 1.15x faster or more)\n",
        "    # The triton kernel isn't compile friendly. We lose time on compilation?\n",
        "    ratio = reference_time / dequant_time\n",
        "    bench.append(ratio)\n",
        "    print(ratio)\n",
        "\n",
        "print(\"Average runtime ratio:\", sum(bench)/len(bench))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVqVEkswYfUi"
      },
      "source": [
        "Misc. Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAPJ2Iy7shXh"
      },
      "source": [
        "### Kernel Parameter Sweep\n",
        "\n",
        "With GPU code, kernel launch parameters can dramatically affect performance. Selecting good parameters can be tricky and input shape dependent.\n",
        "\n",
        "In total we have 3 knobs, `(size, num_warps, num_stages)` in which to tune our parameters in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "krd2reoaNOKE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a46c011-7699-4649-9a75-acce89f8e299"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 1, 1) - Average runtime ratio: 0.4042234886156037\n",
            "(1, 1, 2) - Average runtime ratio: 0.40167460785767295\n",
            "(1, 1, 3) - Average runtime ratio: 0.4017803174926734\n",
            "(1, 1, 4) - Average runtime ratio: 0.40386996123331664\n",
            "(1, 1, 8) - Average runtime ratio: 0.40315240518894363\n",
            "(1, 2, 1) - Average runtime ratio: 0.4026994728289857\n",
            "(1, 2, 2) - Average runtime ratio: 0.4035838934949984\n",
            "(1, 2, 3) - Average runtime ratio: 0.4038324570796806\n",
            "(1, 2, 4) - Average runtime ratio: 0.4038728406943149\n",
            "(1, 2, 8) - Average runtime ratio: 0.4020402684766401\n",
            "(1, 4, 1) - Average runtime ratio: 0.4043805761276771\n",
            "(1, 4, 2) - Average runtime ratio: 0.40425793539666804\n",
            "(1, 4, 3) - Average runtime ratio: 0.4032255738750761\n",
            "(1, 4, 4) - Average runtime ratio: 0.40352173013151166\n",
            "(1, 4, 8) - Average runtime ratio: 0.40377178981418055\n",
            "(2, 1, 1) - Average runtime ratio: 0.6735386318966894\n",
            "(2, 1, 2) - Average runtime ratio: 0.6716827640959635\n",
            "(2, 1, 3) - Average runtime ratio: 0.6697715346858586\n",
            "(2, 1, 4) - Average runtime ratio: 0.6710331042694067\n",
            "(2, 1, 8) - Average runtime ratio: 0.6731521017056735\n",
            "(2, 2, 1) - Average runtime ratio: 0.6731446065657731\n",
            "(2, 2, 2) - Average runtime ratio: 0.6728191833857715\n",
            "(2, 2, 3) - Average runtime ratio: 0.6725231293618747\n",
            "(2, 2, 4) - Average runtime ratio: 0.6720637984091871\n",
            "(2, 2, 8) - Average runtime ratio: 0.6714271494956475\n",
            "(2, 4, 1) - Average runtime ratio: 0.671564930849514\n",
            "(2, 4, 2) - Average runtime ratio: 0.6727043253147261\n",
            "(2, 4, 3) - Average runtime ratio: 0.6728987045969554\n",
            "(2, 4, 4) - Average runtime ratio: 0.6691922479099611\n",
            "(2, 4, 8) - Average runtime ratio: 0.6736417117558692\n",
            "(4, 1, 1) - Average runtime ratio: 1.1777897804666488\n",
            "(4, 1, 2) - Average runtime ratio: 1.1849369870767403\n",
            "(4, 1, 3) - Average runtime ratio: 1.1846551874301154\n",
            "(4, 1, 4) - Average runtime ratio: 1.191552003722407\n",
            "(4, 1, 8) - Average runtime ratio: 1.1797220750048822\n",
            "(4, 2, 1) - Average runtime ratio: 1.181492998006133\n",
            "(4, 2, 2) - Average runtime ratio: 1.185340645036377\n",
            "(4, 2, 3) - Average runtime ratio: 1.1851742890079173\n",
            "(4, 2, 4) - Average runtime ratio: 1.185735596460476\n",
            "(4, 2, 8) - Average runtime ratio: 1.1851339954885998\n",
            "(4, 4, 1) - Average runtime ratio: 1.1895739118306932\n",
            "(4, 4, 2) - Average runtime ratio: 1.1902774100574174\n",
            "(4, 4, 3) - Average runtime ratio: 1.1831649097570165\n",
            "(4, 4, 4) - Average runtime ratio: 1.1830746112391082\n",
            "(4, 4, 8) - Average runtime ratio: 1.1726950851268503\n",
            "(8, 1, 1) - Average runtime ratio: 1.3070982196063994\n",
            "(8, 1, 2) - Average runtime ratio: 1.3060264945458961\n",
            "(8, 1, 3) - Average runtime ratio: 1.3136245728566394\n",
            "(8, 1, 4) - Average runtime ratio: 1.3004943489740886\n",
            "(8, 1, 8) - Average runtime ratio: 1.3127613916195808\n",
            "(8, 2, 1) - Average runtime ratio: 1.307444012020233\n",
            "(8, 2, 2) - Average runtime ratio: 1.3137148448409262\n",
            "(8, 2, 3) - Average runtime ratio: 1.3077178741009576\n",
            "(8, 2, 4) - Average runtime ratio: 1.3185407364898045\n",
            "(8, 2, 8) - Average runtime ratio: 1.308545124243318\n",
            "(8, 4, 1) - Average runtime ratio: 1.311763194593587\n",
            "(8, 4, 2) - Average runtime ratio: 1.298933224354726\n",
            "(8, 4, 3) - Average runtime ratio: 1.2976239213491334\n",
            "(8, 4, 4) - Average runtime ratio: 1.294819865144842\n",
            "(8, 4, 8) - Average runtime ratio: 1.298429750660299\n",
            "(16, 1, 1) - Average runtime ratio: 1.2667147131898586\n",
            "(16, 1, 2) - Average runtime ratio: 1.2555801217896527\n",
            "(16, 1, 3) - Average runtime ratio: 1.2625818132407534\n",
            "(16, 1, 4) - Average runtime ratio: 1.2656059750508286\n",
            "(16, 1, 8) - Average runtime ratio: 1.2566793817009254\n",
            "(16, 2, 1) - Average runtime ratio: 1.2571106352301502\n",
            "(16, 2, 2) - Average runtime ratio: 1.2635912595588445\n",
            "(16, 2, 3) - Average runtime ratio: 1.2774411877933456\n",
            "(16, 2, 4) - Average runtime ratio: 1.2649732855778937\n",
            "(16, 2, 8) - Average runtime ratio: 1.2705064724152162\n",
            "(16, 4, 1) - Average runtime ratio: 1.26506744176378\n",
            "(16, 4, 2) - Average runtime ratio: 1.2621698051173302\n",
            "(16, 4, 3) - Average runtime ratio: 1.2772690423460924\n",
            "(16, 4, 4) - Average runtime ratio: 1.2509623419852531\n",
            "(16, 4, 8) - Average runtime ratio: 1.2711280511196523\n",
            "(32, 1, 1) - Average runtime ratio: 1.2549704122237955\n",
            "(32, 1, 2) - Average runtime ratio: 1.2490433321734757\n",
            "(32, 1, 3) - Average runtime ratio: 1.260766581673301\n",
            "(32, 1, 4) - Average runtime ratio: 1.229944267124984\n",
            "(32, 1, 8) - Average runtime ratio: 1.2385139489161494\n",
            "(32, 2, 1) - Average runtime ratio: 1.2528730097349134\n",
            "(32, 2, 2) - Average runtime ratio: 1.2429685926515432\n",
            "(32, 2, 3) - Average runtime ratio: 1.2540888529691003\n",
            "(32, 2, 4) - Average runtime ratio: 1.251508967398958\n",
            "(32, 2, 8) - Average runtime ratio: 1.261091576688463\n",
            "(32, 4, 1) - Average runtime ratio: 1.2552359568032752\n",
            "(32, 4, 2) - Average runtime ratio: 1.257019834979263\n",
            "(32, 4, 3) - Average runtime ratio: 1.2590412762907848\n",
            "(32, 4, 4) - Average runtime ratio: 1.2475780212239989\n",
            "(32, 4, 8) - Average runtime ratio: 1.2466847973488548\n",
            "(64, 1, 1) - Average runtime ratio: 1.226208916616268\n",
            "(64, 1, 2) - Average runtime ratio: 1.2276362173093565\n",
            "(64, 1, 3) - Average runtime ratio: 1.2265877161112204\n",
            "(64, 1, 4) - Average runtime ratio: 1.2279062989451761\n",
            "(64, 1, 8) - Average runtime ratio: 1.2300748848699383\n",
            "(64, 2, 1) - Average runtime ratio: 1.2164066055996081\n",
            "(64, 2, 2) - Average runtime ratio: 1.222262663609746\n",
            "(64, 2, 3) - Average runtime ratio: 1.231823633497667\n",
            "(64, 2, 4) - Average runtime ratio: 1.2223511086149874\n",
            "(64, 2, 8) - Average runtime ratio: 1.2190365629401991\n",
            "(64, 4, 1) - Average runtime ratio: 1.2278700632388093\n",
            "(64, 4, 2) - Average runtime ratio: 1.2251323437320207\n",
            "(64, 4, 3) - Average runtime ratio: 1.2337537015957418\n",
            "(64, 4, 4) - Average runtime ratio: 1.2236474887145825\n",
            "(64, 4, 8) - Average runtime ratio: 1.2288891916790061\n",
            "(128, 1, 1) - Average runtime ratio: 1.2293359730030264\n",
            "(128, 1, 2) - Average runtime ratio: 1.2321358825343989\n",
            "(128, 1, 3) - Average runtime ratio: 1.2182173348754404\n",
            "(128, 1, 4) - Average runtime ratio: 1.230789282265292\n",
            "(128, 1, 8) - Average runtime ratio: 1.22520596725137\n",
            "(128, 2, 1) - Average runtime ratio: 1.232064574730666\n",
            "(128, 2, 2) - Average runtime ratio: 1.2246070642481717\n",
            "(128, 2, 3) - Average runtime ratio: 1.2113780429333667\n",
            "(128, 2, 4) - Average runtime ratio: 1.2305759485819319\n",
            "(128, 2, 8) - Average runtime ratio: 1.2210380117433755\n",
            "(128, 4, 1) - Average runtime ratio: 1.224771806546359\n",
            "(128, 4, 2) - Average runtime ratio: 1.2227520349090433\n",
            "(128, 4, 3) - Average runtime ratio: 1.2209554247755938\n",
            "(128, 4, 4) - Average runtime ratio: 1.222921632621032\n",
            "(128, 4, 8) - Average runtime ratio: 1.219434693049493\n"
          ]
        }
      ],
      "source": [
        "#  Parameter Sweep\n",
        "RUNS = 1\n",
        "\n",
        "SIZE_DENOM = [1, 2, 4, 8, 16, 32, 64, 128]\n",
        "\n",
        "WARPS = [1,2,4]\n",
        "\n",
        "# -1 = programatically guess the optimal.\n",
        "STAGES = [1, 2, 3, 4, 8]\n",
        "\n",
        "def _your_dequantize_nf4_param(weight, quant_state, size=1, warps=1, stages=1):\n",
        "    ### SETUP TRITON LAUNCH HERE\n",
        "    kernel_dtype = quant_state.dtype\n",
        "    if not HAS_BFLOAT16 and quant_state.dtype == torch.bfloat16:\n",
        "        kernel_dtype = torch.float16 # Coerce to float16 for T4 instance\n",
        "    out = torch.empty(quant_state.shape,\n",
        "                      dtype=kernel_dtype,\n",
        "                      device=weight.device,\n",
        "                      requires_grad = False)\n",
        "    is_transposed = weight.shape[0] == 1\n",
        "    n_out = out.numel()\n",
        "    n_absmax = quant_state.absmax.numel()\n",
        "    n_absmax2 = quant_state.state2.absmax.numel()\n",
        "\n",
        "    ov = out.view(-1)\n",
        "    grid = (n_absmax // size,)\n",
        "\n",
        "    num_blocks = n_absmax // grid[0]\n",
        "    compiled_kernel = _your_dequantize_nf4_kernel[grid](weight, quant_state.absmax,\n",
        "                                      quant_state.state2.absmax, ov,\n",
        "                                      quant_state.code,\n",
        "                                      quant_state.state2.code,\n",
        "                                      num_blocks=num_blocks,\n",
        "                                      num_elements=num_blocks * quant_state.blocksize,\n",
        "                                      n_absmax=n_absmax,\n",
        "                                      n_absmax2=n_absmax2,\n",
        "                                      n_out=n_out,\n",
        "                                      offset=quant_state.offset.item(),\n",
        "                                      kernel_dtype=TORCH_TO_TRITON_DTYPE[kernel_dtype],\n",
        "                                      blocksize=quant_state.blocksize,\n",
        "                                      blocksize2=quant_state.state2.blocksize)\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    out = ov.view(out.shape)\n",
        "    if is_transposed:\n",
        "        return out.transpose()\n",
        "    else:\n",
        "        return out\n",
        "\n",
        "def your_dequantize_nf4_param(weight, size=1, warps=1, stages=1):\n",
        "    return _your_dequantize_nf4_param(weight.weight.data, weight.weight.quant_state, size=size, warps=warps, stages=stages)\n",
        "\n",
        "from functools import partial\n",
        "for sz in SIZE_DENOM:\n",
        "    for warp in WARPS:\n",
        "        for stage in STAGES:\n",
        "            bench = []\n",
        "            parameterized_dequant = partial(your_dequantize_nf4_param, size=sz, warps=warp, stages=stage)\n",
        "            for _ in range(RUNS):\n",
        "                torch.cuda.synchronize()\n",
        "                torch.cuda.empty_cache()\n",
        "                dequant_time = test_dequantize(parameterized_dequant)\n",
        "\n",
        "                torch.cuda.synchronize()\n",
        "                torch.cuda.empty_cache()\n",
        "                reference_time = test_dequantize(unsloth_dequantize)\n",
        "\n",
        "                ### CALCULATE SPEEDUP (hopefully 1.15x faster or more)\n",
        "                # (It's not. It's actually quite subpar. I tried :P)\n",
        "                ratio = reference_time / dequant_time\n",
        "                bench.append(ratio)\n",
        "                print(f\"{(sz, warp, stage)} - Average runtime ratio:\", sum(bench)/len(bench))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the performance of the kernel varies drastically, depending on if we pick good kernel params or not."
      ],
      "metadata": {
        "id": "YIWWuNo_4D8R"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNSaT4GHD9Lz48F6wiViVF0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}