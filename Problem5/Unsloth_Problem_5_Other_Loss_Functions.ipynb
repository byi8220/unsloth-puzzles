{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMOPLIb5OK3GzqlXczTz0w9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/byi8220/unsloth-puzzles/blob/main/Problem5/Unsloth_Problem_5_Other_Loss_Functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unsloth Problem 5 - Memory Efficient Backprop for other loss functions\n",
        "\n",
        "#### Ran on a colab L4 GPU instance with 53 GB VRAM, 24 GB RAM\n",
        "\n",
        "#### Considerations\n",
        "Any degree of chunking will introduce more addition ops, which can lead to compounding numerical error. This is more pronounced along the `qlen` dimension.\n",
        "\n",
        "The selection of `mel_num_chunks` heavily influences memory saved and numerical accuracy. The most stable configuration is `(B,1,1)`, where you only chunk by batch. If you have very large batch sizes, this is sufficient."
      ],
      "metadata": {
        "id": "Xofx2r4Vb_OO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Setup\n",
        "---"
      ],
      "metadata": {
        "id": "c2q5EMNmbcC5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_rx9FYMOc2T"
      },
      "outputs": [],
      "source": [
        "# Code to install Unsloth, Triton, Torch etc\n",
        "%%capture\n",
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n",
        "!pip install --no-deps cut_cross_entropy unsloth_zoo\n",
        "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "!pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EI_d4FLkR51i"
      },
      "outputs": [],
      "source": [
        "# Helpful functions used through the entire notebook\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import set_seed\n",
        "import time\n",
        "import inspect\n",
        "import os\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "HAS_BFLOAT16 = (major_version >= 8)\n",
        "from inspect import currentframe as _C, getframeinfo\n",
        "_F = lambda c: getframeinfo(c).lineno # Gets line number\n",
        "WARN = lambda x: print(f\"\\033[31m{x}\\033[0m\") # Red colored warnings\n",
        "\n",
        "# https://stackoverflow.com/questions/18425225/getting-the-name-of-a-variable-as-a-string\n",
        "def NAME(var):\n",
        "    callers_local_vars = inspect.currentframe().f_back.f_locals.items()\n",
        "    names = [var_name for var_name, var_val in callers_local_vars if var_val is var]\n",
        "    return names[0] if len(names) != 0 else \"\"\n",
        "\n",
        "def assert_same(x, y, line, dtype, atol=None, rtol=None):\n",
        "    assert(x.dtype == dtype)\n",
        "    try: torch.testing.assert_close(x, y, check_stride = True, atol=atol, rtol=rtol)\n",
        "    except Exception as error:\n",
        "        raise RuntimeError(\n",
        "            f\"Failed allclose at line [{line}]: {NAME(x)}, {NAME(y)}\\n{str(error)}\"\n",
        "        )\n",
        "\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<a name=\"problem-5-impl\"><a/>\n",
        "## `MemoryEfficientLinear` Implementation for other functions\n"
      ],
      "metadata": {
        "id": "fwvv3rLgbh10"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rp-IJIbv90f6"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "def transformation_function_with_other_loss(batch, linear, labels,\n",
        "                                            down_projection_function,\n",
        "                                            reduction=\"mean\"):\n",
        "    x = linear(batch).float() # Up projection to large space\n",
        "    loss = down_projection_function(x, labels, reduction=reduction)\n",
        "    return loss\n",
        "\n",
        "# Repeat the test above for various functions\n",
        "def nll_loss_f(batch, labels, reduction=\"mean\"):\n",
        "    \"\"\"The nonlinear part of `transformation_function`\"\"\"\n",
        "    from torch.nn import NLLLoss\n",
        "    # Down projection to small space\n",
        "    down_projection_function = NLLLoss(reduction=reduction)\n",
        "    # These inputs/targets are kinda nonsense, but we're just testing output\n",
        "    # equivalence even with nonsense labels.\n",
        "    batch = F.log_softmax(batch, -1)\n",
        "    loss = down_projection_function(batch.reshape(-1, batch.shape[-1]), labels.reshape(-1))\n",
        "    return loss\n",
        "\n",
        "def bce_loss_f(batch, labels, reduction=\"mean\"):\n",
        "    \"\"\"The nonlinear part of `transformation_function`\"\"\"\n",
        "    from torch.nn import BCELoss\n",
        "    # Down projection to small space\n",
        "    down_projection_function = BCELoss(reduction=reduction)\n",
        "    # These inputs/targets are kinda nonsense, but we're just testing output\n",
        "    # equivalence even with nonsense labels.\n",
        "    loss = down_projection_function(\n",
        "        torch.sigmoid(torch.mean(torch.softmax(batch, -1), -1)),\n",
        "        labels)\n",
        "    return loss\n",
        "\n",
        "def kl_div_loss_f(batch, labels, reduction=\"mean\"):\n",
        "    \"\"\"The nonlinear part of `transformation_function`\"\"\"\n",
        "    from torch.nn import KLDivLoss\n",
        "    # Down projection to small space\n",
        "    down_projection_function = KLDivLoss(reduction=reduction)\n",
        "    # These inputs/targets are kinda nonsense, but we're just testing output\n",
        "    # equivalence even with nonsense labels.\n",
        "    loss = down_projection_function(\n",
        "        torch.sigmoid(torch.mean(torch.softmax(batch, -1), -1)),\n",
        "        labels)\n",
        "    return loss\n",
        "\n",
        "def cross_entropy_f(x, labels, reduction=\"mean\"):\n",
        "    \"\"\"The nonlinear part of `transformation_function`\"\"\"\n",
        "    from torch.nn import CrossEntropyLoss\n",
        "    down_projection_function = CrossEntropyLoss(reduction=reduction)\n",
        "    # Down projection to small space\n",
        "    loss = down_projection_function(x.reshape(-1, x.shape[-1]), labels.reshape(-1))\n",
        "    return loss\n",
        "\n",
        "class MemoryEfficientLinear(torch.autograd.Function):\n",
        "    # IMO, the spec is a bit vague, and I interpreted the arguments to\n",
        "    # as MemoryEfficientLinear.forward(X, W, labels, fn) = fn(XW, labels)\n",
        "    @staticmethod\n",
        "    # (bsz, qlen, hd) @ (hd, vocab) -> (bsz, qlen, vocab)\n",
        "    def forward(ctx, X, W, labels, forward_function, mel_num_chunks=1, ignore_index=-100):\n",
        "        # NOTE: I wasn't sure what `allows_dynamic_chunk_sizes` means here.\n",
        "        # I interpreted it to mean \"let the user specify the number of chunks,\n",
        "        # and the chunks will be sized accordingly.\"\n",
        "        ctx.mel_num_chunks = mel_num_chunks # How to chunk `XW` over batches\n",
        "\n",
        "        # Perform `forward_function` in chunks, and reduce them into `output`\n",
        "        output = 0.0\n",
        "\n",
        "        # Require uniform chunk size, for cleaner computations involving\n",
        "        # `ForCausalLMLoss` and `num_items_in_batch`.\n",
        "        assert X.shape[0] % ctx.mel_num_chunks == 0\n",
        "        assert ctx.mel_num_chunks <= X.shape[0]\n",
        "        b_per_chunk = X.shape[0] // ctx.mel_num_chunks\n",
        "\n",
        "        N = 0\n",
        "        for b in range(ctx.mel_num_chunks):\n",
        "            b0, b1 = b *  b_per_chunk, (b+1) * b_per_chunk\n",
        "            # Reduce (bsz, qlen, vocab) to (b_per_chunk, q_per_chunk, vocab)\n",
        "            with torch.no_grad():\n",
        "                X_slice = X[b0:b1]\n",
        "                l_slice = labels[b0:b1]\n",
        "                XW_slice = (F.linear(X_slice, W.T)).float()\n",
        "            output += torch.numel(l_slice) * forward_function(XW_slice, l_slice)\n",
        "            N += torch.numel(l_slice)\n",
        "        del XW_slice\n",
        "        ctx.save_for_backward(X, W, labels)\n",
        "        ctx.forward_function = forward_function\n",
        "        ctx.N = N\n",
        "        ctx.ignore_index = ignore_index\n",
        "        return output / N\n",
        "\n",
        "    # L(X,W,T,f) = f(XW, T)\n",
        "    # dL/dX = dL/df * df/d(XW) * d(XW)/dX\n",
        "    # dL/dW = dL/df * df/d(XW) * d(XW)/dW\n",
        "    # We want to avoid materializing df/d(XW) to save on memory,\n",
        "    # as XW is the large tensor we are trying to avoid materializing\n",
        "    @staticmethod\n",
        "    def backward(ctx, dY):\n",
        "\n",
        "        # As written we need to retain at least all of X, W, labels\n",
        "        # (This could possibly be optimized more)\n",
        "        X, W, labels = ctx.saved_tensors\n",
        "\n",
        "        # The absolute minimum memory usage this function can possibly incur is\n",
        "        # that required for the returned gradients.\n",
        "        dX = torch.zeros_like(X)\n",
        "        dW = torch.zeros_like(W)\n",
        "        assert X.shape[0] % ctx.mel_num_chunks == 0\n",
        "        assert ctx.mel_num_chunks <= X.shape[0]\n",
        "        b_per_chunk = X.shape[0] // ctx.mel_num_chunks\n",
        "\n",
        "        for b in range(ctx.mel_num_chunks):\n",
        "            b0, b1 = b * b_per_chunk, (b+1) * b_per_chunk\n",
        "            X_slice = X[b0:b1].detach().requires_grad_()\n",
        "            W_slice = W.detach().requires_grad_()\n",
        "            l_slice = labels[b0:b1].detach()\n",
        "            with torch.enable_grad():\n",
        "                XW_slice = (F.linear(X_slice, W_slice.T)).float()\n",
        "                out = ctx.forward_function(XW_slice, l_slice) * torch.numel(l_slice)\n",
        "            # From my testing this appears to use more memory than hardcoded matmul (sometimes)\n",
        "            dX_slice, dW_slice = torch.autograd.grad(out, (X_slice, W_slice), dY / ctx.N, retain_graph=False, create_graph=False)\n",
        "            dX[b0:b1] = dX_slice.to(dX.dtype)\n",
        "            dW += dW_slice.to(dW.dtype)\n",
        "\n",
        "        return dX, dW, None, None, None, None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "## `MemoryEfficientLinear` Tests\n",
        "\n",
        "This just all of Problem 5, Test 1, but repeated for other functions.\n"
      ],
      "metadata": {
        "id": "Qy6ZcU93eNYx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Batch Only\n",
        "\n",
        "#### Functions tested\n",
        "\n",
        "NLLLoss, KLDivLoss, BCELoss - Mean reduction"
      ],
      "metadata": {
        "id": "xfSuLwltR5--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### This could (and for real code, should) be modularized better.\n",
        "# But for the purpose of this problem set, I'm trying to keep things organized\n",
        "# by requirement.\n",
        "import torch\n",
        "from functools import partial\n",
        "import gc\n",
        "\n",
        "# Make sure we're actually using cuda\n",
        "device = 'cuda'\n",
        "\n",
        "# Default config parameters\n",
        "# Smaller than the CE loss test, so I can actually get through them in colab.\n",
        "bsz = 32\n",
        "qlen = 512\n",
        "hd = 512\n",
        "vocab = 32 * 1024 # 32k\n",
        "dtype = torch.bfloat16\n",
        "num_chunks = 8\n",
        "\n",
        "# The projection we are trying to optimize is of the form:\n",
        "# (bsz, qlen, hd) @ (hd, vocab) -> (bsz, qlen, vocab)\n",
        "\n",
        "# Create input\n",
        "\n",
        "batch = torch.randn((bsz, qlen, hd), dtype=dtype, requires_grad=True)\n",
        "batch.retain_grad()\n",
        "ce_labels = torch.randint(0, vocab, (bsz, qlen), dtype=torch.long).to(device)\n",
        "nll_labels = torch.randint(0, vocab, (bsz, qlen), dtype=torch.long).to(device)\n",
        "bce_labels = torch.sigmoid(nll_labels).to(torch.float32).to(device)\n",
        "kl_labels = torch.sigmoid(nll_labels).to(torch.float32).to(device)\n",
        "initial_W = torch.randn(hd, vocab, dtype=dtype)\n",
        "\n",
        "functions_to_test = [cross_entropy_f, nll_loss_f, bce_loss_f, kl_div_loss_f]\n",
        "labels_for_test = [ce_labels, nll_labels, bce_labels, kl_labels]\n",
        "\n",
        "#### Run basic layer\n",
        "for down_projection_function, l in zip(functions_to_test, labels_for_test):\n",
        "    # Initialize a linear layer\n",
        "    base_linear_ = torch.nn.Linear(hd, vocab, bias=False, dtype=dtype).to(device)\n",
        "    base_linear_.weight = torch.nn.Parameter(\n",
        "        initial_W.clone().detach().T.to(device))\n",
        "    base_linear_.weight.grad = None\n",
        "\n",
        "    # Prepare input data\n",
        "    batch_in = batch.clone().detach().to(device).requires_grad_()\n",
        "    batch_in.retain_grad()\n",
        "    batch_in.grad = None\n",
        "\n",
        "    # Clear memory and stats to profile forward\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "    # (bsz, qlen, hd) @ (hd, vocab) -> (bsz, qlen, vocab) logits.\n",
        "    loss_expected = transformation_function_with_other_loss(batch_in, base_linear_, l.detach(), down_projection_function)\n",
        "    base_linear_forward_mem = torch.cuda.max_memory_allocated()\n",
        "\n",
        "    # Without checkpointing, `backward()` doubles our memory usage since we need\n",
        "    # to persist intermediate state.\n",
        "\n",
        "    # Clear memory and stats to profile backward()\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "    # Do the backward\n",
        "    loss_expected.backward()\n",
        "\n",
        "    # Measure peak memory usage\n",
        "    # With the default config (and no checkpointing) we expect this to account for:\n",
        "    # 1. The parameters of base_linear - 4096 * 128k * sizeof(bfloat16) = 1GB\n",
        "    # 2. The upcasted, materialized logits - 2 * 4096 * 128k * sizeof(float) = 4GB\n",
        "    # 3. The memory needed for backprop - At least 4 GB\n",
        "    # 4. The gradients of `base_linear` - 4096 * 128k * sizeof(bfloat16) = 1GB\n",
        "    # 5. `batch` and `labels`, which are negligibly small (Under 200 MB)\n",
        "    # 6. The computed losses, which are just scalars\n",
        "    base_linear_backward_mem = torch.cuda.max_memory_allocated()\n",
        "\n",
        "    # Move to CPU to not interfere VRAM measurement\n",
        "    base_linear_batch_grad_values = batch_in.grad.clone().detach().to('cpu')\n",
        "    base_linear_grad_values = base_linear_.weight.grad.clone().detach().transpose(0,1).to('cpu')\n",
        "\n",
        "    # Free memory\n",
        "    del base_linear_, batch_in\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    #### Run memory efficient layer\n",
        "\n",
        "    # Initialize a linear parameter.\n",
        "    mem_eff_linear_ = torch.nn.Parameter(initial_W.clone().detach().to(device))\n",
        "    memEffLinear = MemoryEfficientLinear.apply\n",
        "    mem_eff_linear_.grad = None\n",
        "\n",
        "    # Prepare input data\n",
        "    batch_in = batch.clone().detach().to(device).requires_grad_()\n",
        "    batch_in.retain_grad()\n",
        "    batch_in.grad = None\n",
        "\n",
        "    # Clear memory and stats to profile forward\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "    loss_actual = memEffLinear(batch_in,\n",
        "                              mem_eff_linear_,\n",
        "                              l.detach(),\n",
        "                              down_projection_function,\n",
        "                              num_chunks)\n",
        "    mem_eff_linear_forward_mem = torch.cuda.max_memory_allocated()\n",
        "\n",
        "    # Clear memory and stats to profile backward()\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "    # Do the backward\n",
        "    loss_actual.backward()\n",
        "\n",
        "    # Measure peak memory usage\n",
        "    mem_eff_linear_backward_mem = torch.cuda.max_memory_allocated()\n",
        "\n",
        "    # Move to CPU to not interfere VRAM measurement\n",
        "    mem_eff_batch_grad_values = batch_in.grad.clone().detach().to('cpu')\n",
        "    mem_eff_linear_grad_values = mem_eff_linear_.grad.clone().detach().to('cpu')\n",
        "\n",
        "    # Free memory\n",
        "    del mem_eff_linear_, memEffLinear, batch_in\n",
        "    torch.cuda.empty_cache()\n",
        "    # Compare forward memory usage from the above test\n",
        "    forward_vram_change = (mem_eff_linear_forward_mem - base_linear_forward_mem) / base_linear_forward_mem\n",
        "\n",
        "    base_linear_forward_vram_gb = (base_linear_forward_mem) / (1024**3)\n",
        "    base_mem_eff_forward_vram_gb = (mem_eff_linear_forward_mem) / (1024**3)\n",
        "    print(\"#### Func {} ####\".format(down_projection_function))\n",
        "    print(\"Peak Memory usage during basic linear forward(): {:.2f} GB\".format(base_linear_forward_vram_gb))\n",
        "    print(\"Peak Memory usage during memory efficient linear forward(): {:.2f} GB\".format(base_mem_eff_forward_vram_gb))\n",
        "    print(\"Change: {:.2f} GB\".format(base_mem_eff_forward_vram_gb - base_linear_forward_vram_gb))\n",
        "    print(\"% Change: {:.2f}%\".format(forward_vram_change * 100))\n",
        "    # Compare backward memory usage from the above test\n",
        "    backward_vram_change = (mem_eff_linear_backward_mem - base_linear_backward_mem) / base_linear_backward_mem\n",
        "\n",
        "    base_linear_backward_vram_gb = (base_linear_backward_mem) / (1024**3)\n",
        "    base_mem_eff_backward_vram_gb = (mem_eff_linear_backward_mem) / (1024**3)\n",
        "    print(\"Peak Memory usage during basic linear backward(): {:.2f} GB\".format(base_linear_backward_vram_gb))\n",
        "    print(\"Peak Memory usage during memory efficient linear backward(): {:.2f} GB\".format(base_mem_eff_backward_vram_gb))\n",
        "    print(\"Change: {:.2f} GB\".format(base_mem_eff_backward_vram_gb - base_linear_backward_vram_gb))\n",
        "    print(\"% Change: {:.2f}%\".format(backward_vram_change * 100))\n",
        "\n",
        "    # Show losses from the above runs are equivalent\n",
        "    # Bfloat16 is quite imprecise: https://nhigham.com/tag/bfloat16/\n",
        "    # \"bfloat16 numbers have the equivalent of about three decimal digits of precision\"\n",
        "    print(\"loss_expected\", loss_expected)\n",
        "    print(\"loss_actual\", loss_actual)\n",
        "    assert_same(loss_expected, loss_actual, _F(_C()), loss_actual.dtype)\n",
        "\n",
        "    # Show gradients are equivalent\n",
        "    # Assert X is same in batch and mem_eff case\n",
        "    assert_same(mem_eff_batch_grad_values, base_linear_batch_grad_values,\n",
        "                _F(_C()), mem_eff_batch_grad_values.dtype)\n",
        "    # Assert W is same in batch and mem_eff case\n",
        "    assert_same(mem_eff_linear_grad_values, base_linear_grad_values,\n",
        "                _F(_C()), mem_eff_linear_grad_values.dtype)\n",
        "    print()"
      ],
      "metadata": {
        "id": "ESVCfezglk7q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e547e12-b9f6-457a-b59a-bafb8c5764b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#### Func <function cross_entropy_f at 0x78df40e5a160> ####\n",
            "Peak Memory usage during basic linear forward(): 4.06 GB\n",
            "Peak Memory usage during memory efficient linear forward(): 0.78 GB\n",
            "Change: -3.27 GB\n",
            "% Change: -80.72%\n",
            "Peak Memory usage during basic linear backward(): 6.06 GB\n",
            "Peak Memory usage during memory efficient linear backward(): 1.24 GB\n",
            "Change: -4.82 GB\n",
            "% Change: -79.57%\n",
            "loss_expected tensor(92.5661, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_actual tensor(92.5661, device='cuda:0', grad_fn=<MemoryEfficientLinearBackward>)\n",
            "\n",
            "#### Func <function nll_loss_f at 0x78df40e0e200> ####\n",
            "Peak Memory usage during basic linear forward(): 4.25 GB\n",
            "Peak Memory usage during memory efficient linear forward(): 0.88 GB\n",
            "Change: -3.38 GB\n",
            "% Change: -79.40%\n",
            "Peak Memory usage during basic linear backward(): 6.16 GB\n",
            "Peak Memory usage during memory efficient linear backward(): 1.33 GB\n",
            "Change: -4.83 GB\n",
            "% Change: -78.39%\n",
            "loss_expected tensor(92.7468, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_actual tensor(92.7469, device='cuda:0', grad_fn=<MemoryEfficientLinearBackward>)\n",
            "\n",
            "#### Func <function bce_loss_f at 0x78df40e0df80> ####\n",
            "Peak Memory usage during basic linear forward(): 4.25 GB\n",
            "Peak Memory usage during memory efficient linear forward(): 0.88 GB\n",
            "Change: -3.38 GB\n",
            "% Change: -79.40%\n",
            "Peak Memory usage during basic linear backward(): 8.25 GB\n",
            "Peak Memory usage during memory efficient linear backward(): 1.58 GB\n",
            "Change: -6.67 GB\n",
            "% Change: -80.84%\n",
            "loss_expected tensor(0.6931, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "loss_actual tensor(0.6931, device='cuda:0', grad_fn=<MemoryEfficientLinearBackward>)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:3384: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#### Func <function kl_div_loss_f at 0x78df40e5a2a0> ####\n",
            "Peak Memory usage during basic linear forward(): 4.25 GB\n",
            "Peak Memory usage during memory efficient linear forward(): 0.88 GB\n",
            "Change: -3.38 GB\n",
            "% Change: -79.40%\n",
            "Peak Memory usage during basic linear backward(): 8.16 GB\n",
            "Peak Memory usage during memory efficient linear backward(): 1.58 GB\n",
            "Change: -6.58 GB\n",
            "% Change: -80.62%\n",
            "loss_expected tensor(-0.5000, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "loss_actual tensor(-0.5000, device='cuda:0', grad_fn=<MemoryEfficientLinearBackward>)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ForCausalLMLoss"
      ],
      "metadata": {
        "id": "CfKoYvC9oGfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test this works on `ForCausalLMLoss\n",
        "import torch\n",
        "from typing import Callable, List, Optional, Tuple, Union\n",
        "from transformers.models.llama.modeling_llama import LlamaForCausalLM, KwargsForCausalLM\n",
        "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
        "from transformers.cache_utils import Cache\n",
        "from transformers.processing_utils import Unpack\n",
        "from transformers.loss.loss_utils import ForCausalLMLoss\n",
        "from functools import partial\n",
        "import torch.nn as nn\n",
        "import gc\n",
        "\n",
        "# Make sure we're actually using cuda\n",
        "device = 'cuda'\n",
        "\n",
        "bsz = 4\n",
        "qlen = 2048\n",
        "hd = 2048\n",
        "vocab = 128256\n",
        "dtype = torch.bfloat16\n",
        "num_chunks = 4\n",
        "# Create input\n",
        "\n",
        "batch = torch.randn((bsz, qlen, hd), dtype=dtype, requires_grad=True)\n",
        "batch.retain_grad()\n",
        "labels = torch.randint(0, vocab, (bsz, qlen), dtype=torch.long).to(device)\n",
        "labels[:,:qlen//4] = -100\n",
        "# labels = torch.randint(0, vocab, (bsz, qlen), dtype=torch.long).to(device)\n",
        "initial_W = torch.randn(hd, vocab, dtype=dtype)\n",
        "\n",
        "# Initialize a linear layer\n",
        "base_linear_ = torch.nn.Linear(hd, vocab, bias=False, dtype=dtype).to(device)\n",
        "base_linear_.weight = torch.nn.Parameter(\n",
        "    initial_W.clone().detach().T.to(device))\n",
        "base_linear_.weight.grad = None\n",
        "\n",
        "# Prepare input data\n",
        "batch_in = batch.clone().detach().to(device).requires_grad_()\n",
        "batch_in.retain_grad()\n",
        "batch_in.grad = None\n",
        "\n",
        "# Clear memory and stats to profile forward\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "# (bsz, qlen, hd) @ (hd, vocab) -> (bsz, qlen, vocab) logits.\n",
        "loss_expected = transformation_function_with_other_loss(batch_in, base_linear_, labels.detach(),\n",
        "                                                        partial(ForCausalLMLoss,\n",
        "                                                                vocab_size=vocab))\n",
        "base_linear_forward_mem = torch.cuda.max_memory_allocated()\n",
        "\n",
        "# Without checkpointing, `backward()` doubles our memory usage since we need\n",
        "# to persist intermediate state.\n",
        "\n",
        "# Clear memory and stats to profile backward()\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "# Do the backward\n",
        "loss_expected.backward()\n",
        "\n",
        "base_linear_backward_mem = torch.cuda.max_memory_allocated()\n",
        "\n",
        "# Move to CPU to not interfere VRAM measurement\n",
        "base_linear_batch_grad_values = batch_in.grad.clone().detach().to('cpu')\n",
        "base_linear_grad_values = base_linear_.weight.grad.clone().detach().transpose(0,1).to('cpu')\n",
        "\n",
        "# Free memory\n",
        "del base_linear_, batch_in\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "#### Run memory efficient layer\n",
        "\n",
        "# Initialize a linear parameter.\n",
        "mem_eff_linear_ = torch.nn.Parameter(initial_W.clone().detach().to(device))\n",
        "memEffLinear = MemoryEfficientLinear.apply\n",
        "mem_eff_linear_.grad = None\n",
        "\n",
        "# Prepare input data\n",
        "batch_in = batch.clone().detach().to(device).requires_grad_()\n",
        "batch_in.retain_grad()\n",
        "batch_in.grad = None\n",
        "\n",
        "# Clear memory and stats to profile forward\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "loss_actual = memEffLinear(batch_in,\n",
        "                              mem_eff_linear_,\n",
        "                              labels.detach(),\n",
        "                              partial(ForCausalLMLoss,vocab_size=vocab),\n",
        "                              num_chunks)\n",
        "mem_eff_linear_forward_mem = torch.cuda.max_memory_allocated()\n",
        "\n",
        "# Clear memory and stats to profile backward()\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "# Do the backward\n",
        "loss_actual.backward()\n",
        "\n",
        "# Measure peak memory usage\n",
        "mem_eff_linear_backward_mem = torch.cuda.max_memory_allocated()\n",
        "\n",
        "# Move to CPU to not interfere VRAM measurement\n",
        "mem_eff_batch_grad_values = batch_in.grad.clone().detach().to('cpu')\n",
        "mem_eff_linear_grad_values = mem_eff_linear_.grad.clone().detach().to('cpu')\n",
        "\n",
        "# Free memory\n",
        "del mem_eff_linear_, memEffLinear, batch_in\n",
        "torch.cuda.empty_cache()\n",
        "# Compare forward memory usage from the above test\n",
        "forward_vram_change = (mem_eff_linear_forward_mem - base_linear_forward_mem) / base_linear_forward_mem\n",
        "\n",
        "base_linear_forward_vram_gb = (base_linear_forward_mem) / (1024**3)\n",
        "base_mem_eff_forward_vram_gb = (mem_eff_linear_forward_mem) / (1024**3)\n",
        "print(\"#### Func {} ####\".format(down_projection_function))\n",
        "print(\"Peak Memory usage during basic linear forward(): {:.2f} GB\".format(base_linear_forward_vram_gb))\n",
        "print(\"Peak Memory usage during memory efficient linear forward(): {:.2f} GB\".format(base_mem_eff_forward_vram_gb))\n",
        "print(\"Change: {:.2f} GB\".format(base_mem_eff_forward_vram_gb - base_linear_forward_vram_gb))\n",
        "print(\"% Change: {:.2f}%\".format(forward_vram_change * 100))\n",
        "# Compare backward memory usage from the above test\n",
        "backward_vram_change = (mem_eff_linear_backward_mem - base_linear_backward_mem) / base_linear_backward_mem\n",
        "\n",
        "base_linear_backward_vram_gb = (base_linear_backward_mem) / (1024**3)\n",
        "base_mem_eff_backward_vram_gb = (mem_eff_linear_backward_mem) / (1024**3)\n",
        "print(\"Peak Memory usage during basic linear backward(): {:.2f} GB\".format(base_linear_backward_vram_gb))\n",
        "print(\"Peak Memory usage during memory efficient linear backward(): {:.2f} GB\".format(base_mem_eff_backward_vram_gb))\n",
        "print(\"Change: {:.2f} GB\".format(base_mem_eff_backward_vram_gb - base_linear_backward_vram_gb))\n",
        "print(\"% Change: {:.2f}%\".format(backward_vram_change * 100))\n",
        "\n",
        "# Show losses from the above runs are equivalent\n",
        "# Bfloat16 is quite imprecise: https://nhigham.com/tag/bfloat16/\n",
        "# \"bfloat16 numbers have the equivalent of about three decimal digits of precision\"\n",
        "print(\"loss_expected\", loss_expected)\n",
        "print(\"loss_actual\", loss_actual)\n",
        "assert_same(loss_expected, loss_actual, _F(_C()), loss_actual.dtype)\n",
        "\n",
        "# Show gradients are equivalent\n",
        "# Assert X is same in batch and mem_eff case\n",
        "assert_same(mem_eff_batch_grad_values, base_linear_batch_grad_values,\n",
        "            _F(_C()), mem_eff_batch_grad_values.dtype)\n",
        "# Assert W is same in batch and mem_eff case\n",
        "assert_same(mem_eff_linear_grad_values, base_linear_grad_values,\n",
        "            _F(_C()), mem_eff_linear_grad_values.dtype)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ljB3GtQoIZk",
        "outputId": "399e85a2-05d9-469f-a319-0751682629ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#### Func <function kl_div_loss_f at 0x78df40e5a2a0> ####\n",
            "Peak Memory usage during basic linear forward(): 8.55 GB\n",
            "Peak Memory usage during memory efficient linear forward(): 4.12 GB\n",
            "Change: -4.43 GB\n",
            "% Change: -51.84%\n",
            "Peak Memory usage during basic linear backward(): 12.37 GB\n",
            "Peak Memory usage during memory efficient linear backward(): 6.51 GB\n",
            "Change: -5.86 GB\n",
            "% Change: -47.37%\n",
            "loss_expected tensor(198.6714, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_actual tensor(198.6715, device='cuda:0', grad_fn=<MemoryEfficientLinearBackward>)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SelectiveLogSoftmax\n",
        "\n",
        "This is for GRPO support, however the signature is rather different from the baseline MemoryEfficientLinear module.\n",
        "\n",
        "**NOTE:** Unfortuantely, this operation isn't very stable in bfloat16. Getting this more precise will require some further thought. However, we might be able to get away with using this for GRPO training anyways."
      ],
      "metadata": {
        "id": "D3IE6o_fHs2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test this works on selective_log_softmax: https://github.com/huggingface/trl/blob/main/trl/trainer/utils.py#L1659\n",
        "import torch\n",
        "from trl.trainer.utils import selective_log_softmax\n",
        "from typing import Callable, List, Optional, Tuple, Union\n",
        "from transformers.models.llama.modeling_llama import LlamaForCausalLM, KwargsForCausalLM\n",
        "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
        "from transformers.cache_utils import Cache\n",
        "from transformers.processing_utils import Unpack\n",
        "from transformers.loss.loss_utils import ForCausalLMLoss\n",
        "from functools import partial\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import gc\n",
        "\n",
        "\n",
        "# Make sure we're actually using cuda\n",
        "device = 'cuda'\n",
        "def transformation_function_for_sls(batch, linear, labels,\n",
        "                                           down_projection_function):\n",
        "    x = linear(batch).float() # Up projection to large space\n",
        "    loss = down_projection_function(x, labels)\n",
        "    return loss\n",
        "\n",
        "# Linear -> selective_log_softmax fusion\n",
        "# This is specialized for the signature of `selective_log_softmax()`\n",
        "class MemoryEfficientLinearSLS(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, X, W, index, forward_function, mel_num_chunks=1):\n",
        "        ctx.mel_num_chunks = mel_num_chunks\n",
        "\n",
        "        assert X.shape[0] % ctx.mel_num_chunks == 0\n",
        "        assert ctx.mel_num_chunks <= X.shape[0]\n",
        "        b_per_chunk = X.shape[0] // ctx.mel_num_chunks\n",
        "        # selective_log_softmax\n",
        "        bsz, qlen = X.shape[0], X.shape[1]\n",
        "        output = torch.zeros(bsz, qlen).to(device)\n",
        "        for b in range(ctx.mel_num_chunks):\n",
        "            b0, b1 = b *  b_per_chunk, (b+1) * b_per_chunk\n",
        "            with torch.no_grad():\n",
        "                X_slice = X[b0:b1]\n",
        "                l_slice = index[b0:b1]\n",
        "                XW_slice = (F.linear(X_slice, W.T)).float()\n",
        "            output[b0:b1] = forward_function(XW_slice, l_slice)\n",
        "        del XW_slice\n",
        "        ctx.save_for_backward(X, W, index)\n",
        "        ctx.forward_function = forward_function\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, dY):\n",
        "\n",
        "        X, W, index = ctx.saved_tensors\n",
        "        dX = torch.zeros_like(X)\n",
        "        dW = torch.zeros_like(W)\n",
        "        assert X.shape[0] % ctx.mel_num_chunks == 0\n",
        "        assert ctx.mel_num_chunks <= X.shape[0]\n",
        "        b_per_chunk = X.shape[0] // ctx.mel_num_chunks\n",
        "        for b in range(ctx.mel_num_chunks):\n",
        "            b0, b1 = b * b_per_chunk, (b+1) * b_per_chunk\n",
        "            X_slice = X[b0:b1].detach().requires_grad_()\n",
        "            W_slice = W.detach().requires_grad_()\n",
        "            l_slice = index[b0:b1].detach()\n",
        "            with torch.enable_grad():\n",
        "                XW_slice = (F.linear(X_slice, W_slice.T)).float()\n",
        "                out = ctx.forward_function(XW_slice, l_slice)\n",
        "            dX_slice, dW_slice = torch.autograd.grad(out, (X_slice, W_slice), dY[b0:b1], retain_graph=False, create_graph=False)\n",
        "            dX[b0:b1] = dX_slice.to(dX.dtype)\n",
        "            dW += dW_slice.to(dW.dtype)\n",
        "        return dX, dW, None, None, None, None\n",
        "\n",
        "bsz = 2\n",
        "qlen = 2048\n",
        "hd = 2048\n",
        "vocab = 128256\n",
        "dtype = torch.bfloat16 # torch.float32 is stable and passes, but bfloat16 is too imprecise\n",
        "num_chunks = 2\n",
        "\n",
        "# Create input\n",
        "batch = torch.randn((bsz, qlen, hd), dtype=dtype, requires_grad=True)\n",
        "batch.retain_grad()\n",
        "input_ids = torch.randint(0, qlen, (bsz, qlen), dtype=torch.long).to(device)\n",
        "initial_W = torch.randn(hd, vocab, dtype=dtype)\n",
        "\n",
        "# Initialize a linear layer\n",
        "base_linear_ = torch.nn.Linear(hd, vocab, bias=False, dtype=dtype).to(device)\n",
        "base_linear_.weight = torch.nn.Parameter(\n",
        "    initial_W.clone().detach().T.to(device))\n",
        "base_linear_.weight.grad = None\n",
        "\n",
        "# Prepare input data\n",
        "batch_in = batch.clone().detach().to(device).requires_grad_()\n",
        "batch_in.retain_grad()\n",
        "batch_in.grad = None\n",
        "\n",
        "# Clear memory and stats to profile forward\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "# (bsz, qlen, hd) @ (hd, vocab) -> (bsz, qlen, vocab) logits.\n",
        "loss_expected = transformation_function_for_sls(batch_in, base_linear_, input_ids.detach(),\n",
        "                                                          selective_log_softmax)\n",
        "base_linear_forward_mem = torch.cuda.max_memory_allocated()\n",
        "\n",
        "# Without checkpointing, `backward()` doubles our memory usage since we need\n",
        "# to persist intermediate state.\n",
        "\n",
        "# Clear memory and stats to profile backward()\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "# Do the backward\n",
        "loss_expected.backward(torch.ones_like(input_ids))\n",
        "\n",
        "base_linear_backward_mem = torch.cuda.max_memory_allocated()\n",
        "\n",
        "# Move to CPU to not interfere VRAM measurement\n",
        "base_linear_batch_grad_values = batch_in.grad.clone().detach().to('cpu')\n",
        "base_linear_grad_values = base_linear_.weight.grad.clone().detach().transpose(0,1).to('cpu')\n",
        "\n",
        "# Free memory\n",
        "del base_linear_, batch_in\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "#### Run memory efficient layer\n",
        "\n",
        "# Initialize a linear parameter.\n",
        "mem_eff_linear_ = torch.nn.Parameter(initial_W.clone().detach().to(device))\n",
        "memEffLinear = MemoryEfficientLinearSLS.apply\n",
        "mem_eff_linear_.grad = None\n",
        "\n",
        "# Prepare input data\n",
        "batch_in = batch.clone().detach().to(device).requires_grad_()\n",
        "batch_in.retain_grad()\n",
        "batch_in.grad = None\n",
        "\n",
        "# Clear memory and stats to profile forward\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "loss_actual = memEffLinear(batch_in,\n",
        "                              mem_eff_linear_,\n",
        "                              input_ids.detach(),\n",
        "                              selective_log_softmax,\n",
        "                              num_chunks)\n",
        "mem_eff_linear_forward_mem = torch.cuda.max_memory_allocated()\n",
        "\n",
        "# Clear memory and stats to profile backward()\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "# Do the backward\n",
        "loss_actual.backward(torch.ones_like(input_ids))\n",
        "\n",
        "# Measure peak memory usage\n",
        "mem_eff_linear_backward_mem = torch.cuda.max_memory_allocated()\n",
        "\n",
        "# Move to CPU to not interfere VRAM measurement\n",
        "mem_eff_batch_grad_values = batch_in.grad.clone().detach().to('cpu')\n",
        "mem_eff_linear_grad_values = mem_eff_linear_.grad.clone().detach().to('cpu')\n",
        "\n",
        "# Free memory\n",
        "del mem_eff_linear_, memEffLinear, batch_in\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Compare forward memory usage from the above test\n",
        "forward_vram_change = (mem_eff_linear_forward_mem - base_linear_forward_mem) / base_linear_forward_mem\n",
        "\n",
        "base_linear_forward_vram_gb = (base_linear_forward_mem) / (1024**3)\n",
        "base_mem_eff_forward_vram_gb = (mem_eff_linear_forward_mem) / (1024**3)\n",
        "\n",
        "print(\"Peak Memory usage during basic linear forward(): {:.2f} GB\".format(base_linear_forward_vram_gb))\n",
        "print(\"Peak Memory usage during memory efficient linear forward(): {:.2f} GB\".format(base_mem_eff_forward_vram_gb))\n",
        "print(\"Change: {:.2f} GB\".format(base_mem_eff_forward_vram_gb - base_linear_forward_vram_gb))\n",
        "print(\"% Change: {:.2f}%\".format(forward_vram_change * 100))\n",
        "# Compare backward memory usage from the above test\n",
        "backward_vram_change = (mem_eff_linear_backward_mem - base_linear_backward_mem) / base_linear_backward_mem\n",
        "\n",
        "base_linear_backward_vram_gb = (base_linear_backward_mem) / (1024**3)\n",
        "base_mem_eff_backward_vram_gb = (mem_eff_linear_backward_mem) / (1024**3)\n",
        "print(\"Peak Memory usage during basic linear backward(): {:.2f} GB\".format(base_linear_backward_vram_gb))\n",
        "print(\"Peak Memory usage during memory efficient linear backward(): {:.2f} GB\".format(base_mem_eff_backward_vram_gb))\n",
        "print(\"Change: {:.2f} GB\".format(base_mem_eff_backward_vram_gb - base_linear_backward_vram_gb))\n",
        "print(\"% Change: {:.2f}%\".format(backward_vram_change * 100))\n",
        "\n",
        "# Show losses from the above runs are equivalent\n",
        "# Bfloat16 is quite imprecise: https://nhigham.com/tag/bfloat16/\n",
        "# \"bfloat16 numbers have the equivalent of about three decimal digits of precision\"\n",
        "print(\"loss_expected\", loss_expected)\n",
        "print(\"loss_actual\", loss_actual)\n",
        "assert_same(loss_expected, loss_actual, _F(_C()), loss_actual.dtype)\n",
        "\n",
        "# Show gradients are equivalent\n",
        "# Assert X is same in batch and mem_eff case\n",
        "assert_same(mem_eff_batch_grad_values, base_linear_batch_grad_values,\n",
        "            _F(_C()), mem_eff_batch_grad_values.dtype)\n",
        "\n",
        "# dW is not numerically stable\n",
        "dw_abs_diff = mem_eff_linear_grad_values - base_linear_grad_values\n",
        "print(\"dW relative difference:\", dw_abs_diff)\n",
        "print(\"min dW difference:\", torch.min(dw_abs_diff))\n",
        "print(\"mean dW difference:\", torch.mean(dw_abs_diff))\n",
        "print(\"max dW difference:\", torch.max(dw_abs_diff))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XD0MQM7fHqFb",
        "outputId": "e8ae726c-dd57-4360-9fd2-b77e85da540a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Peak Memory usage during basic linear forward(): 5.54 GB\n",
            "Peak Memory usage during memory efficient linear forward(): 5.02 GB\n",
            "Change: -0.52 GB\n",
            "% Change: -9.39%\n",
            "Peak Memory usage during basic linear backward(): 7.44 GB\n",
            "Peak Memory usage during memory efficient linear backward(): 6.45 GB\n",
            "Change: -0.99 GB\n",
            "% Change: -13.27%\n",
            "loss_expected tensor([[-247.5000, -247.1250, -180.2837,  ..., -183.6250, -230.3139,\n",
            "         -198.3490],\n",
            "        [-244.7500, -178.7121, -133.5025,  ..., -214.6331, -224.1269,\n",
            "         -202.0156]], device='cuda:0', grad_fn=<SubBackward0>)\n",
            "loss_actual tensor([[-247.5000, -247.1250, -180.2837,  ..., -183.6250, -230.3139,\n",
            "         -198.3490],\n",
            "        [-244.7500, -178.7121, -133.5025,  ..., -214.6331, -224.1269,\n",
            "         -202.0156]], device='cuda:0',\n",
            "       grad_fn=<MemoryEfficientLinearSLSBackward>)\n",
            "dW relative difference: tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00, -2.4414e-04,  0.0000e+00,  ...,  4.1359e-25,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [-1.5625e-02,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  7.8125e-03,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 7.8125e-03,  7.8125e-03,  0.0000e+00,  ..., -2.0680e-25,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [-7.8125e-03,  0.0000e+00,  0.0000e+00,  ...,  4.1359e-25,\n",
            "          0.0000e+00,  0.0000e+00]], dtype=torch.bfloat16)\n",
            "min dW difference: tensor(-0.0625, dtype=torch.bfloat16)\n",
            "mean dW difference: tensor(-2.3167e-08, dtype=torch.bfloat16)\n",
            "max dW difference: tensor(0.0625, dtype=torch.bfloat16)\n"
          ]
        }
      ]
    }
  ]
}