{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm",
      "toc_visible": true,
      "authorship_tag": "ABX9TyOU9b8LByyhn3WOSQ1Wq9RB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/byi8220/unsloth-puzzles/blob/main/Problem3/Unsloth_Problem_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unsloth Problem 3 - Make torch.compile work without graph breaks for QLoRA\n",
        "\n",
        "Run on an Nvidia L4 colab instance (since no bfloat16 on T4).\n",
        "\n",
        "**NOTE:** Funny enough, there's some discussion on this exact problem within https://discuss.pytorch.org/t/how-to-solve-the-graph-break-happen-in-torch-compile/216858/1"
      ],
      "metadata": {
        "id": "Z8g4oRhNhPVa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "F_rx9FYMOc2T"
      },
      "outputs": [],
      "source": [
        "# Code to install Unsloth, Triton, Torch etc\n",
        "# We're installing torch nightly to workaround https://discuss.pytorch.org/t/how-to-solve-the-graph-break-happen-in-torch-compile/216858/1\n",
        "# This leads to some fun version breaks...\n",
        "%%capture\n",
        "!pip install --no-cache-dir --force-reinstall accelerate huggingface_hub datasets trl hf_transfer triton\n",
        "\n",
        "!pip install --no-deps --no-cache-dir --force-reinstall bitsandbytes\n",
        "!pip install --no-cache-dir --force-reinstall transformers==4.49.0\n",
        "\n",
        "!pip install --pre --no-cache-dir --force-reinstall torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu124\n",
        "!pip install --force-reinstall -U numpy\n",
        "!pip install --force-reinstall -U scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EI_d4FLkR51i"
      },
      "outputs": [],
      "source": [
        "# Helpful functions used through the entire notebook\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import set_seed\n",
        "import time\n",
        "import inspect\n",
        "import os\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "HAS_BFLOAT16 = (major_version >= 8)\n",
        "from inspect import currentframe as _C, getframeinfo\n",
        "_F = lambda c: getframeinfo(c).lineno # Gets line number\n",
        "WARN = lambda x: print(f\"\\033[31m{x}\\033[0m\") # Red colored warnings\n",
        "\n",
        "# https://stackoverflow.com/questions/18425225/getting-the-name-of-a-variable-as-a-string\n",
        "def NAME(var):\n",
        "    callers_local_vars = inspect.currentframe().f_back.f_locals.items()\n",
        "    names = [var_name for var_name, var_val in callers_local_vars if var_val is var]\n",
        "    return names[0] if len(names) != 0 else \"\"\n",
        "\n",
        "def assert_same(x, y, line, dtype):\n",
        "    assert(x.dtype == dtype)\n",
        "    try: torch.testing.assert_close(x, y, check_stride = True)\n",
        "    except Exception as error:\n",
        "        raise RuntimeError(\n",
        "            f\"Failed allclose at line [{line}]: {NAME(x)}, {NAME(y)}\\n{str(error)}\"\n",
        "        )\n",
        "\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pukEsR2YnIHQ"
      },
      "source": [
        "---\n",
        "---\n",
        "---\n",
        "<a name=\"COMPILE\"></a>\n",
        "## C) Make `torch.compile` work without graph breaks for QLoRA [Difficulty: Easy to Medium] [Max points: 9]\n",
        "\n",
        "1. Goal: Write a single Python script like task B), except the goal is to `torch.compile` all modules if possible.\n",
        "\n",
        "2. There must NOT be graph breaks, and excessive re-compilations should not be seen.\n",
        "\n",
        "3. You should have say max 30 compilations. Over 60 is definitely wrong.\n",
        "\n",
        "4. The loss must match with the non compiled module.\n",
        "\n",
        "5. Utilize patching as much as possible.\n",
        "\n",
        "6. Think about which areas might need disabling for compilation. Think about regional compilation. How do we compile sections efficiently?\n",
        "\n",
        "7. Log memory / VRAM usage, and monitor speedups as well.\n",
        "\n",
        "8. Must work for QLoRA.\n",
        "\n",
        "We provided a script below, and showcased how to detect if graph breaks are seen. We also torch compiled the MLP for Llama:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QFOXncAVNqmK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch_compile_options = torch_compile_options = {\n",
        "    \"epilogue_fusion\"   : True,\n",
        "    \"max_autotune\"      : True,\n",
        "    \"shape_padding\"     : True,\n",
        "    \"trace.enabled\"     : True,\n",
        "    \"triton.cudagraphs\" : False,\n",
        "}\n",
        "\n",
        "# Enable `fullgraph` to stop processing on graph break.\n",
        "@torch.compile(fullgraph = True, dynamic = True, options = torch_compile_options)\n",
        "def compiled_llama_mlp(self, x):\n",
        "    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
        "    return down_proj\n",
        "\n",
        "import transformers.models.llama.modeling_llama\n",
        "transformers.models.llama.modeling_llama.LlamaMLP.forward = compiled_llama_mlp\n",
        "\n",
        "# Compile flex attn\n",
        "import transformers.integrations.flex_attention\n",
        "transformers.integrations.flex_attention.flex_attention_forward = torch.compile(\n",
        "    transformers.integrations.flex_attention.flex_attention_forward,\n",
        "    fullgraph = True, dynamic = True, options = torch_compile_options\n",
        ")\n",
        "\n",
        "# Compile layernorm\n",
        "transformers.models.llama.modeling_llama.LlamaRMSNorm.forward = torch.compile(\n",
        "    transformers.models.llama.modeling_llama.LlamaRMSNorm.forward,\n",
        "    fullgraph = True, dynamic = True, options = torch_compile_options\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmoQzMDzm1zL",
        "outputId": "1bbff5d5-0eb8-454b-ab76-be96526e8854"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/quantizers/auto.py:206: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
            "  warnings.warn(warning_msg)\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \\\n",
        "    \"expandable_segments:True,\"\\\n",
        "    \"roundup_power2_divisions:[32:256,64:128,256:64,>:32]\"\n",
        "\n",
        "max_seq_length = 1024\n",
        "torch.set_default_dtype(torch.float16)\n",
        "model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
        "dtype = torch.float16\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit              = True,\n",
        "    bnb_4bit_use_double_quant = True,\n",
        "    bnb_4bit_quant_type       = \"nf4\",\n",
        "    bnb_4bit_compute_dtype    = dtype,\n",
        ")\n",
        "\n",
        "model2 = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map = \"auto\",\n",
        "    attn_implementation = \"flex_attention\",\n",
        "    quantization_config = bnb_config,\n",
        "    torch_dtype = dtype, # Need to manually move dtypes\n",
        ")\n",
        "# Compile loss function\n",
        "model2.loss_function = torch.compile(model2.loss_function, fullgraph = True, dynamic = True, options = torch_compile_options)\n",
        "\n",
        "# Need to manually set compute_dtype.\n",
        "setattr(model2.config.quantization_config, \"bnb_4bit_compute_dtype\", dtype)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Get LoRA and setup model\n",
        "lora_config = LoraConfig(\n",
        "    r = 64,\n",
        "    lora_alpha = 128,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    task_type = TaskType.CAUSAL_LM,\n",
        ")\n",
        "\n",
        "model2 = get_peft_model(model2, lora_config)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for name, param in model2.named_parameters():\n",
        "        if \".lora_A.\" in name or \".lora_B.\" in name: param.requires_grad_(True)\n",
        "        else: param.requires_grad_(False)\n",
        "\n",
        "# Currently GC will cause torch.compile to be disabled, so disable it\n",
        "# model.gradient_checkpointing_enable()\n",
        "model2.enable_input_require_grads()\n",
        "\n",
        "# Get dataset\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "url = \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"\n",
        "dataset = load_dataset(\"json\", data_files = {\"train\" : url}, split = \"train[:10%]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbVNGNQ5LlpJ"
      },
      "source": [
        "We provide full logging for `torch.compile` like below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EsekFGdsK5hZ"
      },
      "outputs": [],
      "source": [
        "# Must show all graph breaks are not seen with torch.compile\n",
        "import os\n",
        "os.environ[\"TORCHDYNAMO_VERBOSE\"] = \"1\"\n",
        "os.environ[\"TORCHINDUCTOR_FORCE_DISABLE_CACHES\"] = \"1\"\n",
        "os.environ[\"TORCHINDUCTOR_COMPILE_THREADS\"] = \"1\"\n",
        "\n",
        "import logging\n",
        "torch._inductor.config.debug = True\n",
        "torch._logging.set_logs(\n",
        "    dynamo = logging.WARN,\n",
        "    inductor = logging.WARN,\n",
        "    graph_breaks = True,\n",
        "    recompiles = True,\n",
        "    recompiles_verbose = True,\n",
        "    compiled_autograd_verbose = True,\n",
        "    # aot_joint_graph = True, # Enable for more logs\n",
        "    # aot_graphs = True,\n",
        ")\n",
        "torch._dynamo.config.verbose = True\n",
        "torch._dynamo.config.suppress_errors = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Solution: Patching model to allow compilation\n",
        "\n",
        "Why are there graph breaks? From our stack trace of the unpatched graph above, we see a few lines of interest:\n",
        "\n",
        "```\n",
        "Reason: Unsupported: call_method UserDefinedObjectVariable(Params4bit) t [] {}\n",
        "```\n",
        "\n",
        "```\n",
        "Reason: Unsupported: Graph break due to unsupported builtin None._SimpleCData.__new__. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.\n",
        "```\n",
        "\n",
        "This suggests that in order to fix these graph breaks, we should look into getting custom ops working for 4bit quantized params.\n",
        "\n",
        "That is, we should stop `torch.compile` from tracing in the dequant code and consider it an isolated black box.\n",
        "\n",
        "This may be solvable by wrapping [the code which actually calls the CUDA dequant](https://github.com/bitsandbytes-foundation/bitsandbytes/blob/e772a9e8723cfc2036fecc830c328ad3b9705250/bitsandbytes/functional.py#L1028-L1046) with a custom operator.\n",
        "\n",
        "Which may be hackable with some aggressive monkeypatching somewhere within [MatMul4Bit](https://github.com/bitsandbytes-foundation/bitsandbytes/blob/e772a9e8723cfc2036fecc830c328ad3b9705250/bitsandbytes/autograd/_functions.py#L441).\n"
      ],
      "metadata": {
        "id": "c2bAa0mTK4Oo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first error expands to:\n",
        "```\n",
        "Unsupported: call_method UserDefinedObjectVariable(Params4bit) t [] {}\n",
        "\n",
        "from user code:\n",
        "   File \"<ipython-input-4-1b0083b2d5de>\", line 12, in compiled_llama_mlp\n",
        "    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
        "  File \"/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py\", line 496, in forward\n",
        "    result = self.base_layer(x, *args, **kwargs)\n",
        "  File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py\", line 484, in forward\n",
        "    return bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state).to(inp_dtype)\n",
        "```\n",
        "\n",
        "This suggests that the problem is caused by `Params4bit.t()` when we are trying to use our quantized weights in a matmul.\n",
        "\n",
        "One way of solving this is to:\n",
        "\n",
        "1. Refactor `Params4bit` so that we procure the transposed weights without a graph break\n",
        "2. Refactor+patch `Linear4Bit` to use this new function\n",
        "3. Wrap `dequantize_4bit` in a custom op\n",
        "\n",
        "Maybe since `Params4bit` is a subclass of `Parameter`, we can convince dynamo to stop wrapping it in `UserDefinedObjectVariable`, and instead give it a hint that it can be treated as a `TensorVariable`?"
      ],
      "metadata": {
        "id": "FhZA5rWLP6Dl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom matmul_4bit function\n",
        "import torch\n",
        "from typing import Callable, Optional, Tuple, List\n",
        "import bitsandbytes.functional as F\n",
        "from bitsandbytes.functional import QuantState\n",
        "from bitsandbytes.nn.modules import Params4bit, Linear4bit, fix_4bit_weight_quant_state_from_module\n",
        "import bitsandbytes\n",
        "\n",
        "def _l4b_forward(self, x):\n",
        "    fix_4bit_weight_quant_state_from_module(self)\n",
        "\n",
        "    # weights are cast automatically as Int8Params, but the bias has to be cast manually\n",
        "    if self.bias is not None and self.bias.dtype != x.dtype:\n",
        "        self.bias.data = self.bias.data.to(x.dtype)\n",
        "\n",
        "    if not self.compute_type_is_set:\n",
        "        self.set_compute_type(x)\n",
        "        self.compute_type_is_set = True\n",
        "\n",
        "    inp_dtype = x.dtype\n",
        "    if self.compute_dtype is not None:\n",
        "        x = x.to(self.compute_dtype)\n",
        "    bias = None if self.bias is None else self.bias.to(self.compute_dtype)\n",
        "    weight_tensor = self.weight.data\n",
        "\n",
        "    return bitsandbytes.matmul_4bit(x, weight_tensor.t(), bias=bias, quant_state=self.weight.quant_state).to(inp_dtype)\n",
        "\n",
        "Linear4bit.forward = _l4b_forward\n"
      ],
      "metadata": {
        "id": "Zk4mvCcUQHJs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom dequantize op which doesn't cause graph breaks\n",
        "import torch\n",
        "from typing import Callable, Optional, Tuple, List\n",
        "import bitsandbytes.functional as F\n",
        "from bitsandbytes.functional import QuantState\n",
        "\n",
        "if not '_dequantize_4bit' in globals():\n",
        "    _dequantize_4bit = F.dequantize_4bit\n",
        "\n",
        "@torch.library.custom_op(\"my_qlora::dequantize_4bit\", mutates_args=([\"out\"]))\n",
        "def dequantize_4bit_op(\n",
        "    A: torch.Tensor,\n",
        "    shape: List[int],\n",
        "    absmax: Optional[torch.Tensor] = None,\n",
        "    code: Optional[torch.Tensor] = None,\n",
        "    blocksize: int = 4096,\n",
        "    dtype: torch.dtype = torch.float16,\n",
        "    offset: Optional[torch.Tensor] = None,\n",
        "    absmax2: Optional[torch.Tensor] = None,\n",
        "    code2: Optional[torch.Tensor] = None,\n",
        "    dtype2: Optional[torch.dtype] = None,\n",
        "    blocksize2: Optional[int] = None,\n",
        "    quant_type: str = \"fp4\",\n",
        "    out: torch.Tensor = None, # `out` is not optional (torch.compile doesn't like returning its inputs)\n",
        ") -> None:\n",
        "    # Rebuild quant state\n",
        "    state2 = None\n",
        "    if code2 is not None:\n",
        "        state2 = QuantState(\n",
        "            absmax=absmax2,\n",
        "            blocksize=blocksize2,\n",
        "            code=code2,\n",
        "        )\n",
        "    state = QuantState(\n",
        "        shape=shape,\n",
        "        absmax=absmax,\n",
        "        code=code,\n",
        "        blocksize=blocksize,\n",
        "        dtype=dtype,\n",
        "        offset=offset,\n",
        "        state2=state2\n",
        "    )\n",
        "    _dequantize_4bit(A, state, absmax, out, blocksize, quant_type)\n",
        "\n",
        "# Need to transform `quant_state` into a form accepted by custom ops\n",
        "@torch.compile(fullgraph = True, dynamic = True, options = torch_compile_options)\n",
        "def dequantize_4bit_wrapper(\n",
        "    A: torch.Tensor,\n",
        "    quant_state: Optional[QuantState] = None,\n",
        "    absmax: Optional[torch.Tensor] = None,\n",
        "    out: Optional[torch.Tensor] = None,\n",
        "    blocksize: int = 64,\n",
        "    quant_type: str =\"fp4\",\n",
        "):\n",
        "    # Unpack quant state\n",
        "    if absmax is None:\n",
        "        absmax = quant_state.absmax\n",
        "    code = quant_state.code\n",
        "    blocksize = blocksize if quant_state is None else quant_state.blocksize\n",
        "    dtype = quant_state.dtype\n",
        "    offset = quant_state.offset\n",
        "\n",
        "    state2 = quant_state.state2\n",
        "    absmax2, code2, dtype2, blocksize2 = None, None, None, None\n",
        "    if quant_state.nested:\n",
        "        absmax2, code2, dtype2 = state2.absmax, state2.code, state2.dtype\n",
        "        blocksize2 = blocksize if state2.blocksize is None else state2.blocksize\n",
        "\n",
        "    is_transposed = A.shape[0] == 1\n",
        "    if out is None:\n",
        "        out = torch.empty(quant_state.shape, dtype=quant_state.dtype, device=A.device)\n",
        "    dequantize_4bit_op(\n",
        "        A,\n",
        "        quant_state.shape,\n",
        "        absmax, code, blocksize,\n",
        "        dtype,\n",
        "        offset,\n",
        "        absmax2, code2, dtype2, blocksize2,\n",
        "        quant_type,\n",
        "        out,\n",
        "    )\n",
        "    if is_transposed:\n",
        "        return out.t()\n",
        "    return out\n",
        "\n",
        "F.dequantize_4bit = dequantize_4bit_wrapper # Put our new op in.\n",
        "\n",
        "assert(_dequantize_4bit != F.dequantize_4bit)"
      ],
      "metadata": {
        "id": "Dqawr7xpwWVE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 895
        },
        "outputId": "b9f3f990-2a7b-48e7-9e31-654f658bb2f3",
        "id": "3K0uO0gxdxZj"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "W0410 03:47:20.411000 14548 torch/_inductor/debug.py:454] [1/0] model__1_inference_0 debug trace: /content/torch_compile_debug/run_2025_04_10_03_47_20_295335-pid_14548/torchinductor/model__1_inference_0.0\n",
            "V0410 03:47:20.654000 14548 torch/_dynamo/guards.py:2974] [1/1] [__recompiles_verbose] Recompiling function dequantize_4bit_wrapper in <ipython-input-7-3aa9c7a62eec>:46\n",
            "V0410 03:47:20.654000 14548 torch/_dynamo/guards.py:2974] [1/1] [__recompiles_verbose]     triggered by the following guard failure(s):\n",
            "V0410 03:47:20.654000 14548 torch/_dynamo/guards.py:2974] [1/1] [__recompiles_verbose]     guard 0 failures:\n",
            "V0410 03:47:20.654000 14548 torch/_dynamo/guards.py:2974] [1/1] [__recompiles_verbose]     - 1/0: L['quant_state'].state2.absmax.size()[0] == L['quant_state'].state2.code.size()[0]  # duck sizing added this equality because these variables had the same size 256 (to avoid this specialization, set torch.fx.experimental._config.use_duck_shape = False)\n",
            "W0410 03:47:20.866000 14548 torch/_inductor/debug.py:454] [1/1] model__2_inference_1 debug trace: /content/torch_compile_debug/run_2025_04_10_03_47_20_295335-pid_14548/torchinductor/model__2_inference_1.1\n",
            "W0410 03:47:23.278000 14548 torch/_inductor/utils.py:1213] [3/0] Not enough SMs to use max_autotune_gemm mode\n",
            "W0410 03:47:23.601000 14548 torch/_inductor/debug.py:454] [3/0] model__3_forward_3 debug trace: /content/torch_compile_debug/run_2025_04_10_03_47_20_295335-pid_14548/torchinductor/model__3_forward_3.2\n",
            "W0410 03:47:24.114000 14548 torch/_inductor/debug.py:454] [3/0] model__3_backward_4 debug trace: /content/torch_compile_debug/run_2025_04_10_03_47_20_295335-pid_14548/torchinductor/model__3_backward_4.3\n",
            "V0410 03:47:25.267000 14548 torch/_dynamo/guards.py:2974] [1/2] [__recompiles_verbose] Recompiling function dequantize_4bit_wrapper in <ipython-input-7-3aa9c7a62eec>:46\n",
            "V0410 03:47:25.267000 14548 torch/_dynamo/guards.py:2974] [1/2] [__recompiles_verbose]     triggered by the following guard failure(s):\n",
            "V0410 03:47:25.267000 14548 torch/_dynamo/guards.py:2974] [1/2] [__recompiles_verbose]     guard 0 failures:\n",
            "V0410 03:47:25.267000 14548 torch/_dynamo/guards.py:2974] [1/2] [__recompiles_verbose]     - 1/1: GLOBAL_STATE changed: autocast \n",
            "V0410 03:47:25.267000 14548 torch/_dynamo/guards.py:2974] [1/2] [__recompiles_verbose] \n",
            "V0410 03:47:25.267000 14548 torch/_dynamo/guards.py:2974] [1/2] [__recompiles_verbose]     guard 1 failures:\n",
            "V0410 03:47:25.267000 14548 torch/_dynamo/guards.py:2974] [1/2] [__recompiles_verbose]     - 1/0: GLOBAL_STATE changed: autocast \n",
            "W0410 03:47:25.484000 14548 torch/_inductor/debug.py:454] [1/2] model__5_inference_5 debug trace: /content/torch_compile_debug/run_2025_04_10_03_47_20_295335-pid_14548/torchinductor/model__5_inference_5.4\n",
            "V0410 03:47:25.798000 14548 torch/_dynamo/guards.py:2974] [2/1] [__recompiles_verbose] Recompiling function _flex_attention_hop_wrapper in /usr/local/lib/python3.11/dist-packages/torch/nn/attention/flex_attention.py:1344\n",
            "V0410 03:47:25.798000 14548 torch/_dynamo/guards.py:2974] [2/1] [__recompiles_verbose]     triggered by the following guard failure(s):\n",
            "V0410 03:47:25.798000 14548 torch/_dynamo/guards.py:2974] [2/1] [__recompiles_verbose]     guard 0 failures:\n",
            "V0410 03:47:25.798000 14548 torch/_dynamo/guards.py:2974] [2/1] [__recompiles_verbose]     - 2/0: tensor 'L['args'][0]' size mismatch at index 2. expected 101, actual 261\n",
            "V0410 03:47:26.056000 14548 torch/_dynamo/guards.py:2974] [3/1] [__recompiles_verbose] Recompiling function compiled_llama_mlp in <ipython-input-3-5868a0fa8b30>:11\n",
            "V0410 03:47:26.056000 14548 torch/_dynamo/guards.py:2974] [3/1] [__recompiles_verbose]     triggered by the following guard failure(s):\n",
            "V0410 03:47:26.056000 14548 torch/_dynamo/guards.py:2974] [3/1] [__recompiles_verbose]     guard 0 failures:\n",
            "V0410 03:47:26.056000 14548 torch/_dynamo/guards.py:2974] [3/1] [__recompiles_verbose]     - 3/0: ___check_obj_id(L['self']._modules['up_proj']._modules['base_layer'].compute_type_is_set, 9619232)\n",
            "W0410 03:47:28.513000 14548 torch/_inductor/debug.py:454] [3/1] model__6_forward_7 debug trace: /content/torch_compile_debug/run_2025_04_10_03_47_20_295335-pid_14548/torchinductor/model__6_forward_7.5\n",
            "W0410 03:47:28.996000 14548 torch/_inductor/debug.py:454] [3/1] model__6_backward_8 debug trace: /content/torch_compile_debug/run_2025_04_10_03_47_20_295335-pid_14548/torchinductor/model__6_backward_8.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 00:18, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.538600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.399400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.443300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.437000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.086200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.863500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.089200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.465800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>2.090200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.354500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@title Retrain the model with our patched kernels.\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "torch.cuda.synchronize()\n",
        "peak_memory_before = torch.cuda.max_memory_allocated()\n",
        "trainer = SFTTrainer(\n",
        "    model = model2,\n",
        "    train_dataset = dataset,\n",
        "    processing_class = tokenizer,\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = 1,\n",
        "        gradient_accumulation_steps = 2,\n",
        "        warmup_steps = 1,\n",
        "        max_steps = 10, # Run many steps just so compilation is actually worth it.\n",
        "        logging_steps = 1,\n",
        "        output_dir = \"outputs_new\",\n",
        "        seed = 3407,\n",
        "        max_seq_length = max_seq_length,\n",
        "        fp16 = model2.get_input_embeddings().weight.dtype == torch.float16,\n",
        "        bf16 = model2.get_input_embeddings().weight.dtype == torch.bfloat16,\n",
        "        report_to = \"none\", # For W&B\n",
        "        dataset_num_proc = 4,\n",
        "    ),\n",
        ")\n",
        "patched_stats = trainer.train()\n",
        "\n",
        "torch.cuda.synchronize()\n",
        "peak_memory_after = torch.cuda.max_memory_allocated()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mem_diff = peak_memory_after - peak_memory_before\n",
        "mem_diff_gb = (mem_diff) / (1024**3)\n",
        "\n",
        "print(\"Peak VRAM usage during training: {:.2f} GB\".format(mem_diff_gb))\n",
        "print(\"Train Stats\", patched_stats)"
      ],
      "metadata": {
        "id": "my-M_OQnDfPk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99424b4c-62d8-4c67-d111-237c59c08b46"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Peak VRAM usage during training: 0.87 GB\n",
            "Train Stats TrainOutput(global_step=10, training_loss=2.2767750144004824, metrics={'train_runtime': 31.8307, 'train_samples_per_second': 0.628, 'train_steps_per_second': 0.314, 'total_flos': 10954170839040.0, 'train_loss': 2.2767750144004824})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare with: https://colab.research.google.com/drive/1mzqLo8c9lJ0eewV858qp5zEwx9t2M1gp#scrollTo=my-M_OQnDfPk\n",
        "\n",
        "```\n",
        "# Reference results, with graph breaks\n",
        "Peak VRAM usage during training: 0.88 GB\n",
        "Train Stats TrainOutput(global_step=10, training_loss=2.3851507186889647, metrics={'train_runtime': 62.6108, 'train_samples_per_second': 0.319, 'train_steps_per_second': 0.16, 'total_flos': 10592155496448.0, 'train_loss': 2.3851507186889647})\n",
        "```\n",
        "\n",
        "VRAM usage appears to be the same.\n"
      ],
      "metadata": {
        "id": "1hvM8HqlXqAw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "taQ0kYz3vMEZ"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}